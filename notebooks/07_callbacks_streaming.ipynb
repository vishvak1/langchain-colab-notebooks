{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc077e8-c7d1-4e70-9431-01f32011e5d5",
   "metadata": {},
   "source": [
    "# Callbacks and Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279d007b-7abe-4792-b7c8-01899332df18",
   "metadata": {},
   "source": [
    "What are “Callbacks & Streaming”?\n",
    "\n",
    "- Streaming lets you receive model output token-by-token (or chunk-by-chunk) for real-time UIs and faster feedback.\n",
    "- Callbacks are hooks that fire on events (chain start/end, LLM start/new token/end, tool start/end, retriever, etc.). They enable logging, metrics, tracing, and custom side-effects without tangling your core logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acaff01-73c3-43c8-a700-a810e4fa290c",
   "metadata": {},
   "source": [
    "We'll cover:\n",
    "\n",
    "- Basic token streaming with `streaming=True` and `.stream()`\n",
    "- Built-in `StdOutCallbackHandler`\n",
    "- Writing a custom `BaseCallbackHandler` (latency + token count)\n",
    "- Attaching callbacks globally and per-run with `.with_config(callbacks=[...])`\n",
    "- Callback events across chains, retrievers, and tools (simulated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cecb09-9c7c-4a79-ba73-da0a7409ac02",
   "metadata": {},
   "source": [
    "Run this bootstrap cell before running any subsequent cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238adfe-2918-49c1-a432-4821a1c39d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment & imports\n",
    "import os, time, asyncio\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise RuntimeError(\"Missing OPENAI_API_KEY in .env\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks import BaseCallbackHandler, CallbackManager, StdOutCallbackHandler\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain.tools import tool\n",
    "\n",
    "print(\"✅ Ready: Chat model initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6740d99-400f-428e-bd87-df789d9b0088",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ec3bd-84b5-4db2-bfba-38d9bbc2641f",
   "metadata": {},
   "source": [
    "There are two ways to stream tokens as they're generated\n",
    "\n",
    "- `.stream`: Synchronous -- Works well in plain Python scripts where you don’t want to deal with async/await.\n",
    "- `.astream`: Asyncrhonous -- Useful when you’re inside an async application (FastAPI, async web server, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f2d2e-023c-4cec-9710-94846163c53f",
   "metadata": {},
   "source": [
    "## Asynchronous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a92c8-5970-4bc3-98ea-d9b513721dd4",
   "metadata": {},
   "source": [
    "The `.astream` should be enclosed in an aysynchronous function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16bb41-80ab-4f98-9f28-81a3aa173137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small, fast chat model with streaming enabled\n",
    "llm_stream = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2,\n",
    "    streaming=True,  # critical for real-time token events\n",
    "    callback_manager=CallbackManager([StdOutCallbackHandler()])  # prints tokens as they arrive\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer concisely.\"),\n",
    "    (\"user\", \"List 5 short ideas to test prompt quality.\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm_stream | StrOutputParser()\n",
    "\n",
    "async def streamchain():\n",
    "    async for chunk in chain.astream({}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "# Tokens will \"type out\" thanks to StdOutCallbackHandler\n",
    "print(\"\\n\\n--- Streaming output (async) ---\")\n",
    "await streamchain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa252f26-41d3-4657-ae3a-ed0617a1f9f0",
   "metadata": {},
   "source": [
    "## Synchronous streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd13c8c2-06a4-49fc-a8d9-a4c9011ad73c",
   "metadata": {},
   "source": [
    "Here's the same code but in synchronous streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2eb5b-16e6-48b0-ac73-a290782fc8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_stream = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2,\n",
    "    streaming=True,   # enables token-by-token streaming\n",
    "    callbacks=[StdOutCallbackHandler()]  # optional: prints tokens directly\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer concisely.\"),\n",
    "    (\"user\", \"List 5 short ideas to test prompt quality.\")\n",
    "])\n",
    "\n",
    "# Chain: prompt → llm → output parser\n",
    "chain = prompt | llm_stream | StrOutputParser()\n",
    "\n",
    "print(\"\\n\\n--- Streaming output (sync) ---\")\n",
    "final = []\n",
    "for chunk in chain.stream({}):\n",
    "    # Each chunk is parsed text from the LLM as it arrives\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    final.append(chunk)\n",
    "\n",
    "# Combine chunks into the final result\n",
    "result = \"\".join(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d00deb8-1073-451f-8658-fe6859c67de6",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860a10a-2994-476c-ba75-db7e73be1446",
   "metadata": {},
   "source": [
    "Let’s write a custom handler that records:\n",
    "- wall-clock latency per generation\n",
    "- count of streamed tokens\n",
    "- the final text length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded1cbc-9b70-4878-a1a7-803390289260",
   "metadata": {},
   "source": [
    "The below setup is appended to the model. Meaning that this logging will happen whereever the llm instance is used and not per call (invoke, stream, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351241f-7bd5-4e04-b21f-c3c12b70a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsHandler(BaseCallbackHandler):\n",
    "    # Constructor that inits variables -- Called when object is initialized\n",
    "    def __init__(self, label=\"run\"):\n",
    "        self.label = label # label just for our reference\n",
    "        self.t0 = None # variable to record time (latency)\n",
    "        self.token_count = 0\n",
    "\n",
    "    # Called when an LLM call starts\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        self.t0 = time.time()\n",
    "        self.token_count = 0\n",
    "        print(f\"\\n[{self.label}] LLM start. Prompts={len(prompts)}\")\n",
    "\n",
    "    # Called on each new token (for streaming models)\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        self.token_count += 1\n",
    "\n",
    "    # Called when LLM completes\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        dt = (time.time() - self.t0) * 1000 if self.t0 else None\n",
    "        # response.generations is provider-specific; with ChatOpenAI you'll get AIMessage(s)\n",
    "        try:\n",
    "            text = response.generations[0][0].text\n",
    "        except Exception:\n",
    "            text = getattr(response, \"content\", \"\")\n",
    "        print(f\"[{self.label}] LLM end. ~{self.token_count} tokens streamed. {dt:.0f} ms. Final length={len(text)}\")\n",
    "\n",
    "metrics = MetricsHandler(label=\"demo\")\n",
    "cm = CallbackManager([metrics])\n",
    "\n",
    "llm_obs = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, streaming=True, callback_manager=cm)\n",
    "\n",
    "chain_obs = (ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Be concise.\"),\n",
    "    (\"user\", \"Explain {topic} in <= 60 words.\")\n",
    "]) | llm_obs | StrOutputParser())\n",
    "\n",
    "print(chain_obs.invoke({\"topic\": \"why callbacks are useful\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b55adb-0b23-45f9-af66-3ebf94c16f36",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332dd9b-696b-4fc3-b0d7-a3cf93869f0d",
   "metadata": {},
   "source": [
    "- Subclass `BaseCallbackHandler` and implement the event methods you care about.\n",
    "- Attach via a `CallbackManager([...])` to a model.\n",
    "- The handler records runtime metrics without changing your chain logic.\n",
    "\n",
    "To know more about the callback events like on_llm_start checkout [Langchain's doc on Callback events](https://python.langchain.com/docs/concepts/callbacks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c5731-4a65-4f7a-b4dd-535183b9b349",
   "metadata": {},
   "source": [
    "## Per-runnable or run-scoped callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cedc5-8f04-4124-b198-bc10fd9a629f",
   "metadata": {},
   "source": [
    "If you want to do the above but per call. This is how you'd do it. Run the previous cell before runnnig this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aac90f-e504-4501-b1f0-b5593ff93099",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_handler = MetricsHandler(label=\"one-off\")\n",
    "llm_one_off = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, streaming=True)\n",
    "\n",
    "one_off = (ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Give 4 crisp reasons to add callbacks only for this invocation.\")\n",
    "]) | llm_one_off | StrOutputParser()\n",
    ").with_config(callbacks=[temp_handler])\n",
    "\n",
    "print(one_off.invoke({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30a866-6187-4e71-b4fd-85d0d73c4b54",
   "metadata": {},
   "source": [
    "## Run-scoped callbacks in RunnableParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c35c5-d64b-4f5b-bc89-41b4506ab5a0",
   "metadata": {},
   "source": [
    "When you do the same run-scoped callbacks in RunnableParallel, insteading of scoping to the entire chain the callbacks are scoped to the each branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a22cf3-63d4-4966-8b33-6a301751bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_prompt(style):\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Rewrite in {style} style.\"),\n",
    "        (\"user\", \"{text}\")\n",
    "    ]).partial(style=style)\n",
    "\n",
    "def styled_chain(style, label):\n",
    "    return (style_prompt(style) | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, streaming=True)\n",
    "            | StrOutputParser()).with_config(callbacks=[MetricsHandler(label=label)])\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    formal = styled_chain(\"formal\", \"formal\"),\n",
    "    casual = styled_chain(\"casual\", \"casual\"),\n",
    ")\n",
    "\n",
    "out = parallel.invoke({\"text\": \"callbacks help you observe and debug chains\"})\n",
    "print(\"\\n--- FORMAL ---\\n\", out[\"formal\"])\n",
    "print(\"\\n--- CASUAL ---\\n\", out[\"casual\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06cc62-37d3-4d18-bc01-a3a28c288327",
   "metadata": {},
   "source": [
    "Here, each branch (formal/casual) becomes its own run context, and the callback handler runs independently there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b36ae-4398-4400-9e37-cba652d0f15d",
   "metadata": {},
   "source": [
    "### How it works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33127ddf-1e8c-45f7-825f-525fd56f61f0",
   "metadata": {},
   "source": [
    "- Each branch (formal/casual) has its own handler instance/label.\n",
    "- You’ll see separate start/end logs and token counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349f84d-6732-4647-9d0d-7d740320099b",
   "metadata": {},
   "source": [
    "## Callback Handler for tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfbf80-56c4-404b-befd-bbac82bdb287",
   "metadata": {},
   "source": [
    "Callback handler and callback events can be used for tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47ae7a-1d8e-4f77-ac45-0870877ba8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom handler for tool events ---\n",
    "class ToolishHandler(BaseCallbackHandler):\n",
    "    def on_tool_start(self, serialized, input_str, **kwargs):\n",
    "        print(f\"[tool] start: {serialized.get('name', 'unknown')} input={input_str!r}\")\n",
    "\n",
    "    def on_tool_end(self, output, **kwargs):\n",
    "        print(f\"[tool] end: output={str(output)[:40]!r}\")\n",
    "\n",
    "\n",
    "# --- Define a fake tool using @tool ---\n",
    "@tool(\"knowledge_lookup\", return_direct=True)\n",
    "def fake_tool(q: str) -> str:\n",
    "    \"\"\"Lookup knowledge about a query.\"\"\"\n",
    "    time.sleep(0.3)  # simulate latency\n",
    "    return f\"FAKE_LOOKUP({q}) -> MMR balances relevance with diversity.\"\n",
    "\n",
    "\n",
    "# --- Prompt template ---\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer using ONLY this context:\\n{context}\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "\n",
    "# --- Chain definition ---\n",
    "qa_chain = (\n",
    "    {\"context\": fake_tool, \"question\": lambda x: x[\"q\"]}\n",
    "    | qa_prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, streaming=True)\n",
    "    | StrOutputParser()\n",
    ").with_config(callbacks=[ToolishHandler()])\n",
    "\n",
    "\n",
    "# --- Run ---\n",
    "print(\"\\n--- QA ---\\n\")\n",
    "print(qa_chain.invoke({\"q\": \"What does MMR do?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97268ed-e1f4-4da1-9e0a-315ce729710d",
   "metadata": {},
   "source": [
    "## Log Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb10b870-33be-44c5-8c50-53c2a8fd7ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LangChain, a callback is a mechanism that allows users to hook into the execution of a chain or agent to perform actions at specific points during the process. Callbacks can be used for logging, monitoring, or modifying behavior, enabling more control and flexibility in how chains operate.\n",
      "[ERROR] Chain failed: Question too long for this demo (max 300 chars).\n",
      "[ERROR] Chain failed: Question too long for this demo (max 300 chars).\n",
      "Raised as expected: Question too long for this demo (max 300 chars).\n"
     ]
    }
   ],
   "source": [
    "class ErrorReporter(BaseCallbackHandler):\n",
    "    def on_chain_error(self, error, **kwargs):\n",
    "        print(f\"[ERROR] Chain failed: {error}\")\n",
    "\n",
    "def guard(inputs):\n",
    "    q = inputs.get(\"question\", \"\")\n",
    "    if len(q) > 300:\n",
    "        raise ValueError(\"Question too long for this demo (max 300 chars).\")\n",
    "    return inputs\n",
    "\n",
    "guarded_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Answer briefly: {question}\")\n",
    "])\n",
    "\n",
    "guarded = (\n",
    "    RunnableLambda(guard) \n",
    "    | guarded_prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, streaming=True)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(guarded.with_config(callbacks=[ErrorReporter()]).invoke({\"question\": \"What is a callback in LangChain?\"}))\n",
    "\n",
    "# Trigger an error\n",
    "try:\n",
    "    long_q = \"x\" * 400\n",
    "    guarded.with_config(callbacks=[ErrorReporter()]).invoke({\"question\": long_q})\n",
    "except Exception as e:\n",
    "    print(\"Raised as expected:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1293059-2a67-4818-8a3a-ebaced3684a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
