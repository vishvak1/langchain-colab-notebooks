{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8dc077e8-c7d1-4e70-9431-01f32011e5d5",
      "metadata": {
        "id": "8dc077e8-c7d1-4e70-9431-01f32011e5d5"
      },
      "source": [
        "# Callbacks and Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "279d007b-7abe-4792-b7c8-01899332df18",
      "metadata": {
        "id": "279d007b-7abe-4792-b7c8-01899332df18"
      },
      "source": [
        "What are ‚ÄúCallbacks & Streaming‚Äù?\n",
        "\n",
        "- Streaming lets you receive model output token-by-token (or chunk-by-chunk) for real-time UIs and faster feedback.\n",
        "- Callbacks are hooks that fire on events (chain start/end, LLM start/new token/end, tool start/end, retriever, etc.). They enable logging, metrics, tracing, and custom side-effects without tangling your core logic."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8acaff01-73c3-43c8-a700-a810e4fa290c",
      "metadata": {
        "id": "8acaff01-73c3-43c8-a700-a810e4fa290c"
      },
      "source": [
        "We'll cover:\n",
        "\n",
        "- Basic token streaming with `streaming=True` and `.stream()`\n",
        "- Built-in `StdOutCallbackHandler`\n",
        "- Writing a custom `BaseCallbackHandler` (latency + token count)\n",
        "- Attaching callbacks globally and per-run with `.with_config(callbacks=[...])`\n",
        "- Callback events across chains, retrievers, and tools (simulated)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bootstrap"
      ],
      "metadata": {
        "id": "-r-3rhCKPqf8"
      },
      "id": "-r-3rhCKPqf8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öì--- Before proceeding futher it is very important you do the following: --- üëæ\n",
        "\n",
        "Select the üóù (key) icon in the left pane and include your OpenAI Api key with Name as \"OPENAPI_KEY\" and value as the key, and grant it notebook access in order to be able to run this notebook."
      ],
      "metadata": {
        "id": "YP3P3V-9L9F8"
      },
      "id": "YP3P3V-9L9F8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeddf0b2-fc6a-4479-8d39-6dfaa715bef9"
      },
      "source": [
        "Run the below two cells in the order they are in, before running further cells. Wait till a number appears in place of '*' or '[ ]'. Below the cell you should see \"‚úÖ Ready: Chat model initialized\""
      ],
      "id": "aeddf0b2-fc6a-4479-8d39-6dfaa715bef9"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-openai langchain-community"
      ],
      "metadata": {
        "id": "V37607__LK8B"
      },
      "execution_count": null,
      "outputs": [],
      "id": "V37607__LK8B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7238adfe-2918-49c1-a432-4821a1c39d1e",
      "metadata": {
        "id": "7238adfe-2918-49c1-a432-4821a1c39d1e"
      },
      "outputs": [],
      "source": [
        "# Environment & imports\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "key = userdata.get('OPENAI_API_KEY')  # returns None if not granted\n",
        "if not key:\n",
        "    raise RuntimeError(\"Set OPENAI_API_KEY in a .env file next to this notebook.\")\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.callbacks import BaseCallbackHandler, CallbackManager, StdOutCallbackHandler\n",
        "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
        "from langchain.tools import tool\n",
        "\n",
        "print(\"‚úÖ Ready: Chat model initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6740d99-400f-428e-bd87-df789d9b0088",
      "metadata": {
        "id": "e6740d99-400f-428e-bd87-df789d9b0088"
      },
      "source": [
        "## Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "717ec3bd-84b5-4db2-bfba-38d9bbc2641f",
      "metadata": {
        "id": "717ec3bd-84b5-4db2-bfba-38d9bbc2641f"
      },
      "source": [
        "There are two ways to stream tokens as they're generated\n",
        "\n",
        "- `.stream`: Synchronous -- Works well in plain Python scripts where you don‚Äôt want to deal with async/await.\n",
        "- `.astream`: Asyncrhonous -- Useful when you‚Äôre inside an async application (FastAPI, async web server, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "384f2d2e-023c-4cec-9710-94846163c53f",
      "metadata": {
        "id": "384f2d2e-023c-4cec-9710-94846163c53f"
      },
      "source": [
        "## Asynchronous"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "360a92c8-5970-4bc3-98ea-d9b513721dd4",
      "metadata": {
        "id": "360a92c8-5970-4bc3-98ea-d9b513721dd4"
      },
      "source": [
        "The `.astream` should be enclosed in an aysynchronous function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c16bb41-80ab-4f98-9f28-81a3aa173137",
      "metadata": {
        "id": "8c16bb41-80ab-4f98-9f28-81a3aa173137"
      },
      "outputs": [],
      "source": [
        "# Create a small, fast chat model with streaming enabled\n",
        "llm_stream = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.2,\n",
        "    streaming=True,  # critical for real-time token events\n",
        "    callback_manager=CallbackManager([StdOutCallbackHandler()]),  # prints tokens as they arrive\n",
        "    api_key=key\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer concisely.\"),\n",
        "    (\"user\", \"List 5 short ideas to test prompt quality.\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm_stream | StrOutputParser()\n",
        "\n",
        "async def streamchain():\n",
        "    async for chunk in chain.astream({}):\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "\n",
        "\n",
        "# Tokens will \"type out\" thanks to StdOutCallbackHandler\n",
        "print(\"\\n\\n--- Streaming output (async) ---\")\n",
        "await streamchain()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa252f26-41d3-4657-ae3a-ed0617a1f9f0",
      "metadata": {
        "id": "fa252f26-41d3-4657-ae3a-ed0617a1f9f0"
      },
      "source": [
        "## Synchronous streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd13c8c2-06a4-49fc-a8d9-a4c9011ad73c",
      "metadata": {
        "id": "dd13c8c2-06a4-49fc-a8d9-a4c9011ad73c"
      },
      "source": [
        "Here's the same code but in synchronous streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca2eb5b-16e6-48b0-ac73-a290782fc8bc",
      "metadata": {
        "id": "eca2eb5b-16e6-48b0-ac73-a290782fc8bc"
      },
      "outputs": [],
      "source": [
        "llm_stream = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.2,\n",
        "    streaming=True,   # enables token-by-token streaming\n",
        "    callbacks=[StdOutCallbackHandler()],  # optional: prints tokens directly\n",
        "    api_key=key\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer concisely.\"),\n",
        "    (\"user\", \"List 5 short ideas to test prompt quality.\")\n",
        "])\n",
        "\n",
        "# Chain: prompt ‚Üí llm ‚Üí output parser\n",
        "chain = prompt | llm_stream | StrOutputParser()\n",
        "\n",
        "print(\"\\n\\n--- Streaming output (sync) ---\")\n",
        "final = []\n",
        "for chunk in chain.stream({}):\n",
        "    # Each chunk is parsed text from the LLM as it arrives\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "    final.append(chunk)\n",
        "\n",
        "# Combine chunks into the final result\n",
        "result = \"\".join(final)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d00deb8-1073-451f-8658-fe6859c67de6",
      "metadata": {
        "id": "3d00deb8-1073-451f-8658-fe6859c67de6"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a860a10a-2994-476c-ba75-db7e73be1446",
      "metadata": {
        "id": "a860a10a-2994-476c-ba75-db7e73be1446"
      },
      "source": [
        "Let‚Äôs write a custom handler that records:\n",
        "- wall-clock latency per generation\n",
        "- count of streamed tokens\n",
        "- the final text length"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ded1cbc-9b70-4878-a1a7-803390289260",
      "metadata": {
        "id": "3ded1cbc-9b70-4878-a1a7-803390289260"
      },
      "source": [
        "The below setup is appended to the model. Meaning that this logging will happen whereever the llm instance is used and not per call (invoke, stream, etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d351241f-7bd5-4e04-b21f-c3c12b70a701",
      "metadata": {
        "id": "d351241f-7bd5-4e04-b21f-c3c12b70a701"
      },
      "outputs": [],
      "source": [
        "class MetricsHandler(BaseCallbackHandler):\n",
        "    # Constructor that inits variables -- Called when object is initialized\n",
        "    def __init__(self, label=\"run\"):\n",
        "        self.label = label # label just for our reference\n",
        "        self.t0 = None # variable to record time (latency)\n",
        "        self.token_count = 0\n",
        "\n",
        "    # Called when an LLM call starts\n",
        "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
        "        self.t0 = time.time()\n",
        "        self.token_count = 0\n",
        "        print(f\"\\n[{self.label}] LLM start. Prompts={len(prompts)}\")\n",
        "\n",
        "    # Called on each new token (for streaming models)\n",
        "    def on_llm_new_token(self, token, **kwargs):\n",
        "        self.token_count += 1\n",
        "\n",
        "    # Called when LLM completes\n",
        "    def on_llm_end(self, response, **kwargs):\n",
        "        dt = (time.time() - self.t0) * 1000 if self.t0 else None\n",
        "        # response.generations is provider-specific; with ChatOpenAI you'll get AIMessage(s)\n",
        "        try:\n",
        "            text = response.generations[0][0].text\n",
        "        except Exception:\n",
        "            text = getattr(response, \"content\", \"\")\n",
        "        print(f\"[{self.label}] LLM end. ~{self.token_count} tokens streamed. {dt:.0f} ms. Final length={len(text)}\")\n",
        "\n",
        "metrics = MetricsHandler(label=\"demo\")\n",
        "cm = CallbackManager([metrics])\n",
        "\n",
        "llm_obs = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, streaming=True, callback_manager=cm, api_key=key)\n",
        "\n",
        "chain_obs = (ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Be concise.\"),\n",
        "    (\"user\", \"Explain {topic} in <= 60 words.\")\n",
        "]) | llm_obs | StrOutputParser())\n",
        "\n",
        "print(chain_obs.invoke({\"topic\": \"why callbacks are useful\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54b55adb-0b23-45f9-af66-3ebf94c16f36",
      "metadata": {
        "id": "54b55adb-0b23-45f9-af66-3ebf94c16f36"
      },
      "source": [
        "### How it works"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1332dd9b-696b-4fc3-b0d7-a3cf93869f0d",
      "metadata": {
        "id": "1332dd9b-696b-4fc3-b0d7-a3cf93869f0d"
      },
      "source": [
        "- Subclass `BaseCallbackHandler` and implement the event methods you care about.\n",
        "- Attach via a `CallbackManager([...])` to a model.\n",
        "- The handler records runtime metrics without changing your chain logic.\n",
        "\n",
        "To know more about the callback events like on_llm_start checkout [Langchain's doc on Callback events](https://python.langchain.com/docs/concepts/callbacks/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c3c5731-4a65-4f7a-b4dd-535183b9b349",
      "metadata": {
        "id": "9c3c5731-4a65-4f7a-b4dd-535183b9b349"
      },
      "source": [
        "## Per-runnable or run-scoped callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b2cedc5-8f04-4124-b198-bc10fd9a629f",
      "metadata": {
        "id": "4b2cedc5-8f04-4124-b198-bc10fd9a629f"
      },
      "source": [
        "If you want to do the above but per call. This is how you'd do it. Run the previous cell before runnnig this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26aac90f-e504-4501-b1f0-b5593ff93099",
      "metadata": {
        "id": "26aac90f-e504-4501-b1f0-b5593ff93099"
      },
      "outputs": [],
      "source": [
        "temp_handler = MetricsHandler(label=\"one-off\")\n",
        "llm_one_off = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, streaming=True, api_key=key)\n",
        "\n",
        "one_off = (ChatPromptTemplate.from_messages([\n",
        "    (\"user\", \"Give 4 crisp reasons to add callbacks only for this invocation.\")\n",
        "]) | llm_one_off | StrOutputParser()\n",
        ").with_config(callbacks=[temp_handler])\n",
        "\n",
        "print(one_off.invoke({}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e30a866-6187-4e71-b4fd-85d0d73c4b54",
      "metadata": {
        "id": "9e30a866-6187-4e71-b4fd-85d0d73c4b54"
      },
      "source": [
        "## Run-scoped callbacks in RunnableParallel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "899c35c5-d64b-4f5b-bc89-41b4506ab5a0",
      "metadata": {
        "id": "899c35c5-d64b-4f5b-bc89-41b4506ab5a0"
      },
      "source": [
        "When you do the same run-scoped callbacks in RunnableParallel, insteading of scoping to the entire chain the callbacks are scoped to the each branch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09a22cf3-63d4-4966-8b33-6a301751bf9c",
      "metadata": {
        "id": "09a22cf3-63d4-4966-8b33-6a301751bf9c"
      },
      "outputs": [],
      "source": [
        "def style_prompt(style):\n",
        "    return ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Rewrite in {style} style.\"),\n",
        "        (\"user\", \"{text}\")\n",
        "    ]).partial(style=style)\n",
        "\n",
        "def styled_chain(style, label):\n",
        "    return (style_prompt(style) | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, streaming=True, api_key=key)\n",
        "            | StrOutputParser()).with_config(callbacks=[MetricsHandler(label=label)])\n",
        "\n",
        "parallel = RunnableParallel(\n",
        "    formal = styled_chain(\"formal\", \"formal\"),\n",
        "    casual = styled_chain(\"casual\", \"casual\"),\n",
        ")\n",
        "\n",
        "out = parallel.invoke({\"text\": \"callbacks help you observe and debug chains\"})\n",
        "print(\"\\n--- FORMAL ---\\n\", out[\"formal\"])\n",
        "print(\"\\n--- CASUAL ---\\n\", out[\"casual\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd06cc62-37d3-4d18-bc01-a3a28c288327",
      "metadata": {
        "id": "dd06cc62-37d3-4d18-bc01-a3a28c288327"
      },
      "source": [
        "Here, each branch (formal/casual) becomes its own run context, and the callback handler runs independently there."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f2b36ae-4398-4400-9e37-cba652d0f15d",
      "metadata": {
        "id": "5f2b36ae-4398-4400-9e37-cba652d0f15d"
      },
      "source": [
        "### How it works?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33127ddf-1e8c-45f7-825f-525fd56f61f0",
      "metadata": {
        "id": "33127ddf-1e8c-45f7-825f-525fd56f61f0"
      },
      "source": [
        "- Each branch (formal/casual) has its own handler instance/label.\n",
        "- You‚Äôll see separate start/end logs and token counts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a349f84d-6732-4647-9d0d-7d740320099b",
      "metadata": {
        "id": "a349f84d-6732-4647-9d0d-7d740320099b"
      },
      "source": [
        "## Callback Handler for tools"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edbfbf80-56c4-404b-befd-bbac82bdb287",
      "metadata": {
        "id": "edbfbf80-56c4-404b-befd-bbac82bdb287"
      },
      "source": [
        "Callback handler and callback events can be used for tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e47ae7a-1d8e-4f77-ac45-0870877ba8de",
      "metadata": {
        "id": "2e47ae7a-1d8e-4f77-ac45-0870877ba8de"
      },
      "outputs": [],
      "source": [
        "# --- Custom handler for tool events ---\n",
        "class ToolishHandler(BaseCallbackHandler):\n",
        "    def on_tool_start(self, serialized, input_str, **kwargs):\n",
        "        print(f\"[tool] start: {serialized.get('name', 'unknown')} input={input_str!r}\")\n",
        "\n",
        "    def on_tool_end(self, output, **kwargs):\n",
        "        print(f\"[tool] end: output={str(output)[:40]!r}\")\n",
        "\n",
        "\n",
        "# --- Define a fake tool using @tool ---\n",
        "@tool(\"knowledge_lookup\", return_direct=True)\n",
        "def fake_tool(q: str) -> str:\n",
        "    \"\"\"Lookup knowledge about a query.\"\"\"\n",
        "    time.sleep(0.3)  # simulate latency\n",
        "    return f\"FAKE_LOOKUP({q}) -> MMR balances relevance with diversity.\"\n",
        "\n",
        "\n",
        "# --- Prompt template ---\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer using ONLY this context:\\n{context}\"),\n",
        "    (\"user\", \"{question}\")\n",
        "])\n",
        "\n",
        "\n",
        "# --- Chain definition ---\n",
        "qa_chain = (\n",
        "    {\"context\": fake_tool, \"question\": lambda x: x[\"q\"]}\n",
        "    | qa_prompt\n",
        "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, streaming=True, api_key=key)\n",
        "    | StrOutputParser()\n",
        ").with_config(callbacks=[ToolishHandler()])\n",
        "\n",
        "\n",
        "# --- Run ---\n",
        "print(\"\\n--- QA ---\\n\")\n",
        "print(qa_chain.invoke({\"q\": \"What does MMR do?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e97268ed-e1f4-4da1-9e0a-315ce729710d",
      "metadata": {
        "id": "e97268ed-e1f4-4da1-9e0a-315ce729710d"
      },
      "source": [
        "## Log Errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb10b870-33be-44c5-8c50-53c2a8fd7ffa",
      "metadata": {
        "id": "eb10b870-33be-44c5-8c50-53c2a8fd7ffa"
      },
      "outputs": [],
      "source": [
        "class ErrorReporter(BaseCallbackHandler):\n",
        "    def on_chain_error(self, error, **kwargs):\n",
        "        print(f\"[ERROR] Chain failed: {error}\")\n",
        "\n",
        "def guard(inputs):\n",
        "    q = inputs.get(\"question\", \"\")\n",
        "    if len(q) > 300:\n",
        "        raise ValueError(\"Question too long for this demo (max 300 chars).\")\n",
        "    return inputs\n",
        "\n",
        "guarded_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"user\", \"Answer briefly: {question}\")\n",
        "])\n",
        "\n",
        "guarded = (\n",
        "    RunnableLambda(guard)\n",
        "    | guarded_prompt\n",
        "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, streaming=True, api_key=key)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(guarded.with_config(callbacks=[ErrorReporter()]).invoke({\"question\": \"What is a callback in LangChain?\"}))\n",
        "\n",
        "# Trigger an error\n",
        "try:\n",
        "    long_q = \"x\" * 400\n",
        "    guarded.with_config(callbacks=[ErrorReporter()]).invoke({\"question\": long_q})\n",
        "except Exception as e:\n",
        "    print(\"Raised as expected:\", e)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}