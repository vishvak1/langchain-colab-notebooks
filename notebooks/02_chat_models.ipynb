{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b2c0d0b9-4a0d-4868-9a60-f89a44d45e55",
      "metadata": {
        "id": "b2c0d0b9-4a0d-4868-9a60-f89a44d45e55"
      },
      "source": [
        "# Chat Models in Langchain and Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bootstrap"
      ],
      "metadata": {
        "id": "-GOj0zaoN9G8"
      },
      "id": "-GOj0zaoN9G8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öì--- Before proceeding futher it is very important you do the following: --- üëæ\n",
        "\n",
        "Select the üóù (key) icon in the left pane and include your OpenAI Api key with Name as \"OPENAPI_KEY\" and value as the key, and grant it notebook access in order to be able to run this notebook."
      ],
      "metadata": {
        "id": "YP3P3V-9L9F8"
      },
      "id": "YP3P3V-9L9F8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeddf0b2-fc6a-4479-8d39-6dfaa715bef9"
      },
      "source": [
        "Run the below two cells in the order they are in, before running further cells. Wait till a number appears in place of '*' or '[ ]'. Below the cell you should see \"‚úÖ Ready: Models (ChatOpenAI, OpenAIEmbeddings).\""
      ],
      "id": "aeddf0b2-fc6a-4479-8d39-6dfaa715bef9"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-openai langchain-community"
      ],
      "metadata": {
        "id": "V37607__LK8B"
      },
      "execution_count": null,
      "outputs": [],
      "id": "V37607__LK8B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3195a5c7-f8f5-4a28-a112-f3d96f621574",
      "metadata": {
        "id": "3195a5c7-f8f5-4a28-a112-f3d96f621574"
      },
      "outputs": [],
      "source": [
        "# Bootstrap: environment & imports\n",
        "from google.colab import userdata\n",
        "\n",
        "key = userdata.get('OPENAI_API_KEY')  # returns None if not granted\n",
        "if not key:\n",
        "    raise RuntimeError(\"Set OPENAI_API_KEY in a .env file next to this notebook.\")\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.callbacks import StdOutCallbackHandler, CallbackManager\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "print(\"‚úÖ Ready: Models (ChatOpenAI, OpenAIEmbeddings)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat model"
      ],
      "metadata": {
        "id": "VX_pJ9uDOkDi"
      },
      "id": "VX_pJ9uDOkDi"
    },
    {
      "cell_type": "markdown",
      "id": "5a8f07e5-7cad-4e99-b6f8-4676f0b70360",
      "metadata": {
        "id": "5a8f07e5-7cad-4e99-b6f8-4676f0b70360"
      },
      "source": [
        "Chat Models are a component in Langchain. OpenAI even has it's own configuration in Langchain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee0ea86-7a78-4b2f-b532-80d4286270d1",
      "metadata": {
        "id": "cee0ea86-7a78-4b2f-b532-80d4286270d1"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\", api_key=key\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a concise assistant.\"),\n",
        "    (\"user\", \"List 3 use-cases of chat models for developers.\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "result = chain.invoke({})\n",
        "print(\"\\n--- Final Output ---\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46c29440-2fb0-4ecf-80b6-deeea849db93",
      "metadata": {
        "id": "46c29440-2fb0-4ecf-80b6-deeea849db93"
      },
      "source": [
        "## Configuration and Controlling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "349478ff-e056-4ec7-a465-36a6aa463d8f",
      "metadata": {
        "id": "349478ff-e056-4ec7-a465-36a6aa463d8f"
      },
      "source": [
        "Temperature defines the creativity vs determinism in your model. `streaming=true` part allows you have streaming of chunks if required when you use `.stream`.\n",
        "\n",
        "`max_tokens` is a hard cap on generated tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d541d7a-66fa-4d93-9817-e5b3b9c531fd",
      "metadata": {
        "id": "8d541d7a-66fa-4d93-9817-e5b3b9c531fd"
      },
      "outputs": [],
      "source": [
        "controlled = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.0,     # more deterministic\n",
        "    max_tokens=120,      # hard cap on generated tokens\n",
        "    top_p=1.0,           # nucleus sampling; 1.0 = off\n",
        "    streaming=True,\n",
        "    api_key=key\n",
        ")\n",
        "\n",
        "ctl_chain = (\n",
        "    ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Be precise and brief.\"),\n",
        "        (\"user\", \"Explain {topic} in <= 80 tokens.\")\n",
        "    ])\n",
        "    | controlled\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(ctl_chain.invoke({\"topic\": \"temperature vs top_p\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d4ea16b-e9be-403c-b3b4-9852007fa5fe",
      "metadata": {
        "id": "2d4ea16b-e9be-403c-b3b4-9852007fa5fe"
      },
      "source": [
        "## ChatOpenAI Low-level control"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df832fed-519f-46ff-8f8e-b3f8b415c039",
      "metadata": {
        "id": "df832fed-519f-46ff-8f8e-b3f8b415c039"
      },
      "source": [
        "In ChatOpenAI, you can directly invoke the llm with the Messages rather a PromptTemplate. This way the output containing the metadata and additional information other than the text content can also be accessed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a465f3-6f6d-4363-82b0-507cf6658d15",
      "metadata": {
        "id": "87a465f3-6f6d-4363-82b0-507cf6658d15"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You translate to French, concisely.\"),\n",
        "    HumanMessage(content=\"Translate: 'Good morning, team. Let's start.'\")\n",
        "]\n",
        "\n",
        "raw_resp = llm.invoke(messages)\n",
        "print(\"\\n--- Raw AIMessage ---\")\n",
        "print(type(raw_resp), raw_resp)\n",
        "\n",
        "print(\"\\n--- As text ---\")\n",
        "print(raw_resp.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f485d4d-fb76-4002-a491-f521d57028f3",
      "metadata": {
        "id": "7f485d4d-fb76-4002-a491-f521d57028f3"
      },
      "source": [
        "## Fallbacks and Timeouts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c73d2bc7-8b06-4276-912b-1592ddca4539",
      "metadata": {
        "id": "c73d2bc7-8b06-4276-912b-1592ddca4539"
      },
      "source": [
        "Use `.with_fallbacks([...])` to try backup models if the primary fails. Use `.with_config(timeout=...)` for per-run time limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e59eedd2-7c92-4249-9231-a4d20502874c",
      "metadata": {
        "id": "e59eedd2-7c92-4249-9231-a4d20502874c"
      },
      "outputs": [],
      "source": [
        "primary = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, api_key=key)\n",
        "backup  = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8, api_key=key)  # in practice use a different provider/model\n",
        "\n",
        "robust = (ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer crisply.\"),\n",
        "    (\"user\", \"{q}\")\n",
        "]) | primary).with_config(timeout= 5).with_fallbacks([backup]) | StrOutputParser()\n",
        "\n",
        "print(robust.invoke({\"q\": \"Give me 3 bullets on LCEL.\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f5a5c42-d72e-49b5-bf4d-4bc883c414c0",
      "metadata": {
        "id": "4f5a5c42-d72e-49b5-bf4d-4bc883c414c0"
      },
      "source": [
        "The timeout can be configured in the `.invoke` stage as well. Like this"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a949f439-5074-4240-a105-11628f9a03e6",
      "metadata": {
        "id": "a949f439-5074-4240-a105-11628f9a03e6"
      },
      "source": [
        "```python\n",
        "print(robust.invoke({\"q\": \"Give me 3 bullets on LCEL.\"}, config={\"timeout\": 15}))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cbf2f36-6fd5-4f8e-9f13-ad1a27b1f179",
      "metadata": {
        "id": "9cbf2f36-6fd5-4f8e-9f13-ad1a27b1f179"
      },
      "source": [
        "## Embedding Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d87166-d80d-451e-b7b7-239045d3504e",
      "metadata": {
        "id": "79d87166-d80d-451e-b7b7-239045d3504e"
      },
      "source": [
        "Apart from the llms used in the application, embedding models are another type of models that help in converting chunks of text into vectors that can be stored in a vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d423b493-5b16-4c91-9f71-2a9906220469",
      "metadata": {
        "id": "d423b493-5b16-4c91-9f71-2a9906220469"
      },
      "outputs": [],
      "source": [
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=key)  # fast & cheap for learning\n",
        "\n",
        "texts = [\n",
        "    \"LangChain composes LLM apps using runnables and chains.\",\n",
        "    \"Vector embeddings map text to numeric vectors for similarity.\",\n",
        "    \"Bananas are a good source of potassium.\"\n",
        "]\n",
        "\n",
        "vecs = emb.embed_documents(texts)\n",
        "query = \"How do I represent text for similarity search?\"\n",
        "qvec = emb.embed_query(query)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}