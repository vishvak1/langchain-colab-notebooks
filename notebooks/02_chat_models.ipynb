{"cells":[{"cell_type":"markdown","id":"b2c0d0b9-4a0d-4868-9a60-f89a44d45e55","metadata":{"id":"b2c0d0b9-4a0d-4868-9a60-f89a44d45e55"},"source":["# Chat Models in Langchain and Configuration"]},{"cell_type":"markdown","source":["## Bootstrap"],"metadata":{"id":"-GOj0zaoN9G8"},"id":"-GOj0zaoN9G8"},{"cell_type":"markdown","source":["⚓--- Before proceeding futher it is very important you do the following: --- 👾\n","\n","Select the 🗝 (key) icon in the left pane and include your OpenAI Api key with Name as \"OPENAPI_KEY\" and value as the key, and grant it notebook access in order to be able to run this notebook."],"metadata":{"id":"YP3P3V-9L9F8"},"id":"YP3P3V-9L9F8"},{"cell_type":"markdown","metadata":{"id":"aeddf0b2-fc6a-4479-8d39-6dfaa715bef9"},"source":["Run the below two cells in the order they are in, before running further cells. Wait till a number appears in place of '*' or '[ ]'. Below the cell you should see \"Ready. LangChain + OpenAI set up.\""],"id":"aeddf0b2-fc6a-4479-8d39-6dfaa715bef9"},{"cell_type":"code","source":["!pip install -q langchain langchain-openai langchain-community pydantic pypdf faiss-cpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V37607__LK8B","executionInfo":{"status":"ok","timestamp":1756410618099,"user_tz":-330,"elapsed":20828,"user":{"displayName":"Vishvak","userId":"11352816944170122427"}},"outputId":"bad325a5-9905-4661-c8f7-0d2dbfbd2000"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"id":"V37607__LK8B"},{"cell_type":"code","execution_count":3,"id":"3195a5c7-f8f5-4a28-a112-f3d96f621574","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3195a5c7-f8f5-4a28-a112-f3d96f621574","executionInfo":{"status":"ok","timestamp":1756410632472,"user_tz":-330,"elapsed":2792,"user":{"displayName":"Vishvak","userId":"11352816944170122427"}},"outputId":"fa29deff-189f-4813-af8a-963440340adf"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Ready: Models (ChatOpenAI, OpenAIEmbeddings)\n"]}],"source":["# Bootstrap: environment & imports\n","import os\n","from google.colab import userdata\n","\n","key = userdata.get('OPENAI_API_KEY')  # returns None if not granted\n","if not key:\n","    raise RuntimeError(\"Set OPENAI_API_KEY in a .env file next to this notebook.\")\n","\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.callbacks import StdOutCallbackHandler, CallbackManager\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","from langchain_core.runnables import RunnableLambda\n","\n","print(\"✅ Ready: Models (ChatOpenAI, OpenAIEmbeddings)\")"]},{"cell_type":"markdown","source":["## Chat model"],"metadata":{"id":"VX_pJ9uDOkDi"},"id":"VX_pJ9uDOkDi"},{"cell_type":"markdown","id":"5a8f07e5-7cad-4e99-b6f8-4676f0b70360","metadata":{"id":"5a8f07e5-7cad-4e99-b6f8-4676f0b70360"},"source":["Chat Models are a component in Langchain. OpenAI even has it's own configuration in Langchain."]},{"cell_type":"code","execution_count":null,"id":"cee0ea86-7a78-4b2f-b532-80d4286270d1","metadata":{"id":"cee0ea86-7a78-4b2f-b532-80d4286270d1"},"outputs":[],"source":["llm = ChatOpenAI(\n","    model=\"gpt-4o-mini\", api_key=key\n",")\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a concise assistant.\"),\n","    (\"user\", \"List 3 use-cases of chat models for developers.\")\n","])\n","\n","chain = prompt | llm | StrOutputParser()\n","result = chain.invoke({})\n","print(\"\\n--- Final Output ---\")\n","print(result)"]},{"cell_type":"markdown","id":"46c29440-2fb0-4ecf-80b6-deeea849db93","metadata":{"id":"46c29440-2fb0-4ecf-80b6-deeea849db93"},"source":["## Configuration and Controlling"]},{"cell_type":"markdown","id":"349478ff-e056-4ec7-a465-36a6aa463d8f","metadata":{"id":"349478ff-e056-4ec7-a465-36a6aa463d8f"},"source":["Temperature defines the creativity vs determinism in your model. `streaming=true` part allows you have streaming of chunks if required when you use `.stream`.\n","\n","`max_tokens` is a hard cap on generated tokens."]},{"cell_type":"code","execution_count":null,"id":"8d541d7a-66fa-4d93-9817-e5b3b9c531fd","metadata":{"id":"8d541d7a-66fa-4d93-9817-e5b3b9c531fd"},"outputs":[],"source":["controlled = ChatOpenAI(\n","    model=\"gpt-4o-mini\",\n","    temperature=0.0,     # more deterministic\n","    max_tokens=120,      # hard cap on generated tokens\n","    top_p=1.0,           # nucleus sampling; 1.0 = off\n","    streaming=True,\n","    api_key=key\n",")\n","\n","ctl_chain = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"Be precise and brief.\"),\n","        (\"user\", \"Explain {topic} in <= 80 tokens.\")\n","    ])\n","    | controlled\n","    | StrOutputParser()\n",")\n","\n","print(ctl_chain.invoke({\"topic\": \"temperature vs top_p\"}))"]},{"cell_type":"markdown","id":"2d4ea16b-e9be-403c-b3b4-9852007fa5fe","metadata":{"id":"2d4ea16b-e9be-403c-b3b4-9852007fa5fe"},"source":["## ChatOpenAI Low-level control"]},{"cell_type":"markdown","id":"df832fed-519f-46ff-8f8e-b3f8b415c039","metadata":{"id":"df832fed-519f-46ff-8f8e-b3f8b415c039"},"source":["In ChatOpenAI, you can directly invoke the llm with the Messages rather a PromptTemplate. This way the output containing the metadata and additional information other than the text content can also be accessed."]},{"cell_type":"code","execution_count":null,"id":"87a465f3-6f6d-4363-82b0-507cf6658d15","metadata":{"id":"87a465f3-6f6d-4363-82b0-507cf6658d15"},"outputs":[],"source":["messages = [\n","    SystemMessage(content=\"You translate to French, concisely.\"),\n","    HumanMessage(content=\"Translate: 'Good morning, team. Let's start.'\")\n","]\n","\n","raw_resp = llm.invoke(messages)\n","print(\"\\n--- Raw AIMessage ---\")\n","print(type(raw_resp), raw_resp)\n","\n","print(\"\\n--- As text ---\")\n","print(raw_resp.content)"]},{"cell_type":"markdown","id":"7f485d4d-fb76-4002-a491-f521d57028f3","metadata":{"id":"7f485d4d-fb76-4002-a491-f521d57028f3"},"source":["## Fallbacks and Timeouts"]},{"cell_type":"markdown","id":"c73d2bc7-8b06-4276-912b-1592ddca4539","metadata":{"id":"c73d2bc7-8b06-4276-912b-1592ddca4539"},"source":["Use `.with_fallbacks([...])` to try backup models if the primary fails. Use `.with_config(timeout=...)` for per-run time limits."]},{"cell_type":"code","execution_count":null,"id":"e59eedd2-7c92-4249-9231-a4d20502874c","metadata":{"id":"e59eedd2-7c92-4249-9231-a4d20502874c"},"outputs":[],"source":["primary = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n","backup  = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8)  # in practice use a different provider/model\n","\n","robust = (ChatPromptTemplate.from_messages([\n","    (\"system\", \"Answer crisply.\"),\n","    (\"user\", \"{q}\")\n","]) | primary).with_config(timeout= 5).with_fallbacks([backup]) | StrOutputParser()\n","\n","print(robust.invoke({\"q\": \"Give me 3 bullets on LCEL.\"}))"]},{"cell_type":"markdown","id":"4f5a5c42-d72e-49b5-bf4d-4bc883c414c0","metadata":{"id":"4f5a5c42-d72e-49b5-bf4d-4bc883c414c0"},"source":["The timeout can be configured in the `.invoke` stage as well. Like this"]},{"cell_type":"markdown","id":"a949f439-5074-4240-a105-11628f9a03e6","metadata":{"id":"a949f439-5074-4240-a105-11628f9a03e6"},"source":["```python\n","print(robust.invoke({\"q\": \"Give me 3 bullets on LCEL.\"}, config={\"timeout\": 15}))\n","```"]},{"cell_type":"markdown","id":"9cbf2f36-6fd5-4f8e-9f13-ad1a27b1f179","metadata":{"id":"9cbf2f36-6fd5-4f8e-9f13-ad1a27b1f179"},"source":["## Embedding Models"]},{"cell_type":"markdown","id":"79d87166-d80d-451e-b7b7-239045d3504e","metadata":{"id":"79d87166-d80d-451e-b7b7-239045d3504e"},"source":["Apart from the llms used in the application, embedding models are another type of models that help in converting chunks of text into vectors that can be stored in a vector database."]},{"cell_type":"code","execution_count":null,"id":"d423b493-5b16-4c91-9f71-2a9906220469","metadata":{"id":"d423b493-5b16-4c91-9f71-2a9906220469"},"outputs":[],"source":["emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # fast & cheap for learning\n","\n","texts = [\n","    \"LangChain composes LLM apps using runnables and chains.\",\n","    \"Vector embeddings map text to numeric vectors for similarity.\",\n","    \"Bananas are a good source of potassium.\"\n","]\n","\n","vecs = emb.embed_documents(texts)\n","query = \"How do I represent text for similarity search?\"\n","qvec = emb.embed_query(query)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}