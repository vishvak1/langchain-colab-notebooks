{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6006e541-596a-4eb8-9fa6-f3c3bbb5716e",
      "metadata": {
        "id": "6006e541-596a-4eb8-9fa6-f3c3bbb5716e"
      },
      "source": [
        "# Introduction - Runnables and Chains (LCEL)\n",
        "\n",
        "Learn the Langchain Expression Language (LCEL) by composing Runnables into Chains using the | operator. You'll build prompt -> model -> parser flows, stream flows, fan out inputs, run branches in parallel, and compare LCEL to the legacy `SequentialChain`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef0d02c0-4e62-4198-a201-c3d644ec435f",
      "metadata": {
        "id": "ef0d02c0-4e62-4198-a201-c3d644ec435f"
      },
      "source": [
        "## Bootstrap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "âš“--- Before proceeding futher it is very important you do the following: --- ðŸ‘¾\n",
        "\n",
        "Select the ðŸ— (key) icon in the left pane and include your OpenAI Api key with Name as \"OPENAPI_KEY\" and value as the key, and grant it notebook access in order to be able to run this notebook."
      ],
      "metadata": {
        "id": "YP3P3V-9L9F8"
      },
      "id": "YP3P3V-9L9F8"
    },
    {
      "cell_type": "markdown",
      "id": "aeddf0b2-fc6a-4479-8d39-6dfaa715bef9",
      "metadata": {
        "id": "aeddf0b2-fc6a-4479-8d39-6dfaa715bef9"
      },
      "source": [
        "Run the below two cells in the order they are in, before running further cells. Wait till a number appears in place of '*' or '[ ]'. Below the cell you should see \"Ready. LangChain + OpenAI set up.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-openai langchain-community"
      ],
      "metadata": {
        "id": "V37607__LK8B"
      },
      "id": "V37607__LK8B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30dea728-2414-41d0-840b-68bde2719953",
      "metadata": {
        "id": "30dea728-2414-41d0-840b-68bde2719953"
      },
      "outputs": [],
      "source": [
        "# 1) Imports & env\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "key = userdata.get('OPENAI_API_KEY')  # returns None if not granted\n",
        "if not key:\n",
        "    raise RuntimeError(\"Set OPENAI_API_KEY in a .env file next to this notebook.\")\n",
        "\n",
        "# 2) LangChain core\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 3) OpenAI chat model via LangChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 4) Streaming helpers\n",
        "from langchain_core.callbacks import StdOutCallbackHandler, CallbackManager\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",           # pick a small, fast model for learning\n",
        "    temperature=0.2,\n",
        "    streaming=True,                # critical for real-time tokens\n",
        "    # callbacks can be set here or passed per-invoke\n",
        "    callback_manager=CallbackManager([StdOutCallbackHandler()]),\n",
        "    api_key=key,\n",
        ")\n",
        "\n",
        "print(\"Ready. LangChain + OpenAI set up.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482ee9df-ccb7-4ce5-9625-e2cc6f9d299a",
      "metadata": {
        "id": "482ee9df-ccb7-4ce5-9625-e2cc6f9d299a"
      },
      "source": [
        "## Starting with a Simple LCEL Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68baece3-4392-4a03-8325-c518d924b968",
      "metadata": {
        "id": "68baece3-4392-4a03-8325-c518d924b968"
      },
      "source": [
        "Here's a simple LCEL chain that utilizes core components from Langchain like PromptTemplates and Output parsers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02787e25-d07e-4ec1-8b1f-2382c94d540d",
      "metadata": {
        "id": "02787e25-d07e-4ec1-8b1f-2382c94d540d"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a concise assistant.\"),\n",
        "        (\"user\", \"Explain {topic} in 3 bullet points.\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Single request (no streaming output capture needed; the callback already prints as tokens)\n",
        "result = chain.invoke({\"topic\": \"LangChain Expression Language (LCEL)\"})\n",
        "print(\"\\n\\nFinal text:\\n\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757ce664-d1f2-4552-ac07-6f03839c3c6f",
      "metadata": {
        "id": "757ce664-d1f2-4552-ac07-6f03839c3c6f"
      },
      "source": [
        "### How it works\n",
        "\n",
        "1. **PromptTemplates**: PromptTemplate enables dynamic insertion of variables into standardized prompt structures. Here, the `ChatPromptTemplate` defines the system/user messages with a placeholder `{topic}`.\n",
        "2. **Model**: The model is the `llm` we are using, it can be any llm even sourced llms hosted on Ollama. Here in our case, it is ChatGPT 4o mini for which we have a separate library support by Langchain `langchain_openai`. In the configuration of our model in the previous cell, we have set `streaming=true` so tokens appear as they are produced.\n",
        "3. **OutputParser**: OutputParsers are a way of ensuring the output of the llm is structured in the way you are expecting them, it can vary from unstructured formats like String or structured formats like XML, JSON, CSV, etc. Here we are using `StrOutputParser` which ensures the returned text is a String of text.\n",
        "4. **LCEL Chain:** LangChain Expression Language (LCEL) is a declarative way of composing chains with different components superceding the traditional `SequentialChain`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e341589b-c8da-4f1e-a8c4-2b3d5a51b338",
      "metadata": {
        "id": "e341589b-c8da-4f1e-a8c4-2b3d5a51b338"
      },
      "source": [
        "## Streams"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e238a68-4f3a-49f1-9167-4a2654b91795",
      "metadata": {
        "id": "4e238a68-4f3a-49f1-9167-4a2654b91795"
      },
      "source": [
        "Let's say you want to create a chatbot where the output must appear as they are being typed or any other scenario where you want to stream the tokens as they are generated. While we cannot completely stream the tokens as they are generated by the LLMs, we can create an impression like it's being streamed using `.stream`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b20fc9-c140-45e7-aea8-5ed318a937f0",
      "metadata": {
        "id": "a1b20fc9-c140-45e7-aea8-5ed318a937f0"
      },
      "outputs": [],
      "source": [
        "# Any LCEL runnable supports .stream() which yields chunks\n",
        "chunks = chain.stream({\"topic\": \"Runnables vs Chains in LangChain\"})\n",
        "buffer = []\n",
        "for chunk in chunks:\n",
        "    # chunk is text here (post StrOutputParser)\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "    buffer.append(chunk)\n",
        "final_text = \"\".join(buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d791661f-60fa-4cd5-aa8b-3a2d6a46b30d",
      "metadata": {
        "id": "d791661f-60fa-4cd5-aa8b-3a2d6a46b30d"
      },
      "source": [
        "## Runnable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d02740a8-a818-4c3e-abf9-408dc5c0fde8",
      "metadata": {
        "id": "d02740a8-a818-4c3e-abf9-408dc5c0fde8"
      },
      "source": [
        "Before going any further it is imperative you understand the **Runnable** interface that acts as the core interface for Langchain components. It provides a standardized way of interacting with the components.\n",
        "\n",
        "The Runnable interface defines three main methods for execution:\n",
        "\n",
        "- `.invoke()`: This method is for synchronous execution. It takes a single input and returns a single output. It's best used when you're running a single chain or need to get a result immediately without parallelism.\n",
        "\n",
        "- `.batch()`: This method is also for synchronous execution, but it takes a list of inputs and returns a list of outputs. It's highly efficient because it can process all inputs in parallel, which is useful when you have many tasks to complete.\n",
        "\n",
        "- `.stream()`: This method is for streaming execution. It takes a single input and returns a stream of outputs (an iterator). This is especially useful for large language models (LLMs) and other components that can generate output piece by piece. It allows you to display the results to the user as they are generated, providing a better user experience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c088117d-73cc-4471-a59e-81b099a49fb2",
      "metadata": {
        "id": "c088117d-73cc-4471-a59e-81b099a49fb2"
      },
      "source": [
        "Runnable components can come together to create a chain which is also a Runnable instance. That means, in the code\n",
        "\n",
        "```python\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "```\n",
        "\n",
        "All the components including the chain itself is a Runnable component."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "711a2695-3306-4343-aa24-dddca6ed0da1",
      "metadata": {
        "id": "711a2695-3306-4343-aa24-dddca6ed0da1"
      },
      "source": [
        "## Runnable Lambda"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc3b46a1-d50d-48cc-a73f-ef46ef78fdc4",
      "metadata": {
        "id": "bc3b46a1-d50d-48cc-a73f-ef46ef78fdc4"
      },
      "source": [
        "RunnableLambda wraps your Python functions as a Runnable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "874d9733-ba0a-411f-8419-e5eb4aba717d",
      "metadata": {
        "id": "874d9733-ba0a-411f-8419-e5eb4aba717d"
      },
      "outputs": [],
      "source": [
        "def get_time(_):\n",
        "    return time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def normalize_question(x):\n",
        "    return x[\"question\"].strip().rstrip(\"?\") + \"?\"\n",
        "\n",
        "time_r = RunnableLambda(get_time)\n",
        "norm_q = RunnableLambda(normalize_question)\n",
        "\n",
        "multi_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are helpful and concise. Current time: {now}\"),\n",
        "    (\"user\", \"{question}\")\n",
        "])\n",
        "\n",
        "multi_chain = ({\"now\": time_r, \"question\": norm_q}) | multi_prompt | llm | StrOutputParser()\n",
        "\n",
        "result = multi_chain.invoke({\"question\": \" what is a Runnable  \"})\n",
        "print(\"\\n--- Final Output ---\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0e889c6-5c4a-4c10-9cef-938877d88e35",
      "metadata": {
        "id": "c0e889c6-5c4a-4c10-9cef-938877d88e35"
      },
      "source": [
        "Here, two instances are created:\n",
        "\n",
        "- `time_r` - wraps `get_time()` to provide current timestamp\n",
        "- `norm_q` - wraps `normalize_question()` to clean up question formatting\n",
        "\n",
        "Now, that the Python code has been converted into a Runnable it can be used to compose chains."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc74dce-67ef-40fc-bfd6-d3effdcec51b",
      "metadata": {
        "id": "dcc74dce-67ef-40fc-bfd6-d3effdcec51b"
      },
      "source": [
        "### Composing inputs and Dictionaries\n",
        "\n",
        "If you look the `.invoke` or `.stream` used so far. All of them include a dictionary. This dictionary is passed as the input to the chain upon say `.invoke`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46b67662-991a-42d8-8a9e-40796f435b64",
      "metadata": {
        "id": "46b67662-991a-42d8-8a9e-40796f435b64"
      },
      "source": [
        "```python\n",
        "{\"now\": time_r, \"question\": norm_q}\n",
        "```\n",
        "\n",
        "In the above case, When you create a dictionary with Runnable values like this, LangChain automatically converts it into a **RunnableParallel**, meaning:\n",
        "- `time_r` and `norm_q` execute simultaneously (in parallel)\n",
        "- Both functions receive the same input dictionary: `{\"question\": \" what is a Runnable  \"}`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4afe2570-0074-4612-ada8-c4b15dd17217",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "4afe2570-0074-4612-ada8-c4b15dd17217"
      },
      "source": [
        "## RunnableParallel to get a Dictionary of results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e639b3b6-db02-4c2b-b2dc-d5b79c69c6b7",
      "metadata": {
        "id": "e639b3b6-db02-4c2b-b2dc-d5b79c69c6b7"
      },
      "source": [
        "You can run two (or more) chains in parallel and get a dict of results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf68f1a6-f874-451d-b15c-b83d28ace5cf",
      "metadata": {
        "id": "cf68f1a6-f874-451d-b15c-b83d28ace5cf"
      },
      "outputs": [],
      "source": [
        "style_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Rewrite the user content in {style} style.\"),\n",
        "    (\"user\", \"{content}\")\n",
        "])\n",
        "style_chain = style_prompt | llm | StrOutputParser()\n",
        "\n",
        "parallel = RunnableParallel(\n",
        "    formal=({\"style\": RunnableLambda(lambda _: \"formal\"),\n",
        "             \"content\": RunnableLambda(lambda x: x[\"text\"])}) | style_chain,\n",
        "    casual=({\"style\": RunnableLambda(lambda _: \"casual\"),\n",
        "             \"content\": RunnableLambda(lambda x: x[\"text\"])}) | style_chain,\n",
        ")\n",
        "\n",
        "outputs = parallel.invoke({\"text\": \"please explain LCEL briefly\"})\n",
        "print(\"\\n--- FORMAL ---\\n\", outputs[\"formal\"])\n",
        "print(\"\\n--- CASUAL ---\\n\", outputs[\"casual\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08c260d8-a904-4ac8-aedf-ee4ed85c05d0",
      "metadata": {
        "id": "08c260d8-a904-4ac8-aedf-ee4ed85c05d0"
      },
      "source": [
        "If you are confused by the RunnableLambda here, It's simply wrapping anonymous functions in Python as Runnable. `lambda` is the keyword to create anonymous functions.\n",
        "\n",
        "- `lambda _: \"formal\"` is an anonymous function that ignores any input and presents the output as \"formal\". _ is the universal symbol in programming of ignoring input parameters. Langchain pipeline always passes data to a Runnable, so a Runnable should be able to accept input parameters even if it is not going to use them.\n",
        "- `lambda x: x[\"text\"]` is another anonymous function that takes in an input dictionary and returns the value for the key \"text\" in that dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa17060-8546-4a61-a4e5-e30d06301c47",
      "metadata": {
        "id": "6fa17060-8546-4a61-a4e5-e30d06301c47"
      },
      "source": [
        "Here the chains wrapped around `RunnableParallel` are executed simultaneously and not sequentially. To confirm this yourself you can use `.stream` insteado of invoke here and you will get the chunks parallely and outputting them will show you that \"-- FORMAL --\" and \"-- CASUAL --\" appear simultaneously. **Try it as an exercise in the below cell:**\n",
        "> Hint: The chunks will not be text this time and the output won't be coherent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57836b67-b960-47cd-bad3-fa6e4e8f341d",
      "metadata": {
        "id": "57836b67-b960-47cd-bad3-fa6e4e8f341d"
      },
      "outputs": [],
      "source": [
        "stream_outputs = parallel.stream({\"text\": \"please explain LCEL briefly\"})\n",
        "\n",
        "## Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2d73c87-816d-4c34-8430-5e9235f3096b",
      "metadata": {
        "id": "b2d73c87-816d-4c34-8430-5e9235f3096b"
      },
      "source": [
        "## Map and Batch processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ada65e2-0582-4770-a38c-20772b5e4c92",
      "metadata": {
        "id": "6ada65e2-0582-4770-a38c-20772b5e4c92"
      },
      "source": [
        "You can setup a chain to take a list of inputs and return a list of outputs using `.map` at the end of your chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cec076f-c86b-413d-9b8b-f423cee3220b",
      "metadata": {
        "id": "4cec076f-c86b-413d-9b8b-f423cee3220b"
      },
      "outputs": [],
      "source": [
        "items = []\n",
        "for t in [\"Runnables\", \"LCEL\", \"Output Parsers\"]:\n",
        "    items.append({\"topic\": t}) # Creates a list of dictionaries with key \"topic\"\n",
        "\n",
        "mapped = (prompt | llm | StrOutputParser()).map()   # list-in â†’ list-out\n",
        "map_results = mapped.invoke(items)\n",
        "\n",
        "print(\"\\n--- MAPPED RESULTS ---\")\n",
        "print(\"\\n---\\n\".join(map_results))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5686d5a7-17cd-46e6-91f6-9e94e22d9870",
      "metadata": {
        "id": "5686d5a7-17cd-46e6-91f6-9e94e22d9870"
      },
      "source": [
        "This can also be done using .batch without .map which also serves a similar purpose (to take a list of inputs and return a list of outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d318c258-c6e0-45a4-9827-5f3857fb4ef2",
      "metadata": {
        "id": "d318c258-c6e0-45a4-9827-5f3857fb4ef2"
      },
      "outputs": [],
      "source": [
        "mapped_batch = prompt | llm | StrOutputParser()\n",
        "batch_results = mapped_batch.batch(items)\n",
        "print(\"\\n--- BATCHED RESULTS ---\")\n",
        "print(\"\\n---\\n\".join(batch_results))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "956f6d4e-448e-4001-a382-5eb85e4c9be0",
      "metadata": {
        "id": "956f6d4e-448e-4001-a382-5eb85e4c9be0"
      },
      "source": [
        "## Legacy SequentialChain vs LCEL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b53384-cbe6-49e9-8e89-236753fc802b",
      "metadata": {
        "id": "f4b53384-cbe6-49e9-8e89-236753fc802b"
      },
      "source": [
        "Let's consider the following example:\n",
        "\n",
        "You have a company idea you need to a company name and a slogan based on the company name and the idea. Let's do this using SequentialChain and LCEL to illustrate the power of LCEL and why declarative way of composing chains is easier."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71b89173-f7bc-493f-b053-0ccb1233a300",
      "metadata": {
        "id": "71b89173-f7bc-493f-b053-0ccb1233a300"
      },
      "source": [
        "### SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22261262-d611-42d7-aa64-19f18c13c349",
      "metadata": {
        "id": "22261262-d611-42d7-aa64-19f18c13c349"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain, SequentialChain  # legacy-style chains\n",
        "\n",
        "# Step 1: Company name\n",
        "prompt_template_name = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You respond in one word.\"),\n",
        "        (\"user\", \"What is a creative and memorable name for a company that sells {product}?\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "name_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_template_name,\n",
        "    output_key=\"company_name\"\n",
        ")\n",
        "\n",
        "# Step 2: Slogan (consumes `company_name`)\n",
        "prompt_template_slogan = PromptTemplate.from_template(\n",
        "    template=\"Write a catchy and short slogan for a company named '{company_name}'.\",\n",
        ")\n",
        "\n",
        "slogan_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_template_slogan,\n",
        "    output_key=\"slogan\"\n",
        ")\n",
        "\n",
        "# Step 3: Combine with SequentialChain\n",
        "sequential_chain = SequentialChain(\n",
        "    chains=[name_chain, slogan_chain],\n",
        "    input_variables=[\"product\"],\n",
        "    output_variables=[\"company_name\", \"slogan\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "product_idea = \"eco-friendly bamboo toothbrushes\"\n",
        "result = sequential_chain({\"product\": product_idea})\n",
        "\n",
        "print(\"\\n--- Final Output ---\")\n",
        "print(f\"For a company selling '{product_idea}':\")\n",
        "print(f\"Suggested Company Name: {result['company_name'].strip()}\")\n",
        "print(f\"Suggested Slogan: {result['slogan'].strip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193882a8-7485-41e6-abad-b8b4f034d8d6",
      "metadata": {
        "id": "193882a8-7485-41e6-abad-b8b4f034d8d6"
      },
      "source": [
        "### LCEL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b2c610-c2f8-4b6c-b25d-91b051c28552",
      "metadata": {
        "id": "01b2c610-c2f8-4b6c-b25d-91b051c28552"
      },
      "source": [
        "Make sure you run the previous cell before running the below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "770bf59c-8b58-49e3-b49d-d86af819a1d9",
      "metadata": {
        "id": "770bf59c-8b58-49e3-b49d-d86af819a1d9"
      },
      "outputs": [],
      "source": [
        "# Step A: a Runnable that produces a company name\n",
        "company_name = (\n",
        "    prompt_template_name\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Step B: build a dict state with the tagline\n",
        "# Then feed it to a second prompt for the slogan\n",
        "lcel_chain = {\n",
        "    \"company_name\": company_name\n",
        "} | {\n",
        "    \"company_name\": RunnableLambda(lambda x: x[\"company_name\"]),\n",
        "    \"slogan\": (\n",
        "        prompt_template_slogan\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "}\n",
        "\n",
        "out = lcel_chain.invoke({\"product\": \"eco-friendly bamboo toothbrushes\"})\n",
        "print(\"\\n--- LCEL OUTPUT ---\")\n",
        "print(\"Company Name:\", out[\"company_name\"].strip())\n",
        "print(\"Slogan:\", out[\"slogan\"].strip())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}