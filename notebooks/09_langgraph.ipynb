{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f76796a-b446-4283-9c2e-ee87b0840d4f",
      "metadata": {
        "id": "8f76796a-b446-4283-9c2e-ee87b0840d4f"
      },
      "source": [
        "# LangGraph — Core Concepts & Hands-On"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is LangGraph?\n",
        "- LangGraph helps you build stateful, multi-step LLM workflows by drawing a little graph in code.\n",
        "\n",
        "Components in Langraph:\n",
        "\n",
        "- A State: a shared “notebook” (a Python dictionary) where steps write/read variables.\n",
        "\n",
        "- Nodes: steps (Python functions) that read the state and return updates.\n",
        "\n",
        "- Edges: arrows that say which node runs next.\n",
        "\n",
        "- Entry point: where the graph starts.\n",
        "\n",
        "- END: where it stops.\n",
        "\n",
        "- Checkpointing: optional saving of state between runs (so you can resume a conversation).\n",
        "\n",
        "- Streaming: watch nodes run, step by step.\n",
        "\n",
        "What we’ll cover:\n",
        "\n",
        "- Minimal linear graph (2 nodes)\n",
        "\n",
        "- Conversational agent graph that can call tools (calculator/search)\n",
        "\n",
        "- Branching with a router (conditional edges)\n",
        "\n",
        "- Checkpointing + Streaming\n",
        "\n",
        "- What each LangGraph component is and why it exists"
      ],
      "metadata": {
        "id": "Jmh0-0UM8VlX"
      },
      "id": "Jmh0-0UM8VlX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bootstrap"
      ],
      "metadata": {
        "id": "-r-3rhCKPqf8"
      },
      "id": "-r-3rhCKPqf8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚓--- Before proceeding futher it is very important you do the following: --- 👾\n",
        "\n",
        "Select the 🗝 (key) icon in the left pane and include your OpenAI Api key with Name as \"OPENAPI_KEY\" and value as the key, and grant it notebook access in order to be able to run this notebook."
      ],
      "metadata": {
        "id": "YP3P3V-9L9F8"
      },
      "id": "YP3P3V-9L9F8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeddf0b2-fc6a-4479-8d39-6dfaa715bef9"
      },
      "source": [
        "Run the below two cells in the order they are in, before running further cells. Wait till a number appears in place of '*' or '[ ]'. Below the cell you should see \"✅ Setup complete\"."
      ],
      "id": "aeddf0b2-fc6a-4479-8d39-6dfaa715bef9"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q -U langchain langchain-openai langgraph"
      ],
      "metadata": {
        "id": "yA2R9-ro88aI"
      },
      "id": "yA2R9-ro88aI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment: load your API key from secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "key = userdata.get('OPENAI_API_KEY')  # returns None if not granted\n",
        "if not key:\n",
        "    raise RuntimeError(\"Set OPENAI_API_KEY in a .env file next to this notebook.\")\n",
        "\n",
        "# LangChain (model & messages)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "\n",
        "# LangGraph core pieces\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "from langgraph.graph.message import MessagesState  # a ready-made \"state shape\" for chat: {\"messages\": [...]}\n",
        "from langgraph.checkpoint.memory import MemorySaver # simple in-memory checkpointing\n",
        "from langgraph.prebuilt import ToolNode              # a helper node that can execute tools for you\n",
        "\n",
        "# Tools decorator (to declare a function as a \"tool\" the model can call)\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "print(\"✅ Setup complete\")"
      ],
      "metadata": {
        "id": "AdJNotmu9BxF"
      },
      "id": "AdJNotmu9BxF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Components in LangGraph"
      ],
      "metadata": {
        "id": "fIHd4lnT9mGO"
      },
      "id": "fIHd4lnT9mGO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### State\n",
        "\n",
        "Think of State as a shared dictionary.\n",
        "Example shape: {\"topic\": \"...\", \"draft\": \"...\", \"summary\": \"...\"}.\n",
        "\n",
        "Every node can read from state and return updates (a small dict) to merge back."
      ],
      "metadata": {
        "id": "1y2zUGmi9sUD"
      },
      "id": "1y2zUGmi9sUD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Node\n",
        "\n",
        "A Node is a Python function that takes the current state and returns a dict with new or updated keys."
      ],
      "metadata": {
        "id": "3dRxqsH19xGG"
      },
      "id": "3dRxqsH19xGG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Edge\n",
        "\n",
        "An Edge connects one node to the next. You’ll define a start node and how the flow moves, until END."
      ],
      "metadata": {
        "id": "TXIi71ke98Zt"
      },
      "id": "TXIi71ke98Zt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Langgraph example"
      ],
      "metadata": {
        "id": "_CHAIF_B-By1"
      },
      "id": "_CHAIF_B-By1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a Langgraph example where for a given topic a draft is created and then the draft is summarized."
      ],
      "metadata": {
        "id": "l-wiWvPw-B6r"
      },
      "id": "l-wiWvPw-B6r"
    },
    {
      "cell_type": "code",
      "source": [
        "# A small model to generate text\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, api_key=key)\n",
        "\n",
        "# 1) Define a \"State shape\" using a simple Python type comment for clarity:\n",
        "# We'll store: topic (string), draft (string), summary (string)\n",
        "# In Python you can just treat the state as a dict with these keys.\n",
        "\n",
        "# 2) Node 1: create a draft from the topic\n",
        "def draft_node(state: dict) -> dict:\n",
        "    topic = state[\"topic\"]\n",
        "    draft = llm.invoke([(\"user\", f\"Write a short (80-120 words) draft about: {topic}\")]).content\n",
        "    return {\"draft\": draft}\n",
        "\n",
        "# 3) Node 2: summarize the draft\n",
        "def summarize_node(state: dict) -> dict:\n",
        "    draft = state[\"draft\"]\n",
        "    summary = llm.invoke([(\"user\", f\"Summarize in exactly 3 bullets:\\n\\n{draft}\")]).content\n",
        "    return {\"summary\": summary}\n",
        "\n",
        "# 4) Build the graph\n",
        "g = StateGraph(dict)        # we tell LangGraph \"our state is a dict\"\n",
        "g.add_node(\"draft\", draft_node)         # add the first node, give it a name\n",
        "g.add_node(\"summarize\", summarize_node) # add the second node\n",
        "\n",
        "g.set_entry_point(\"draft\")              # where the graph starts\n",
        "g.add_edge(\"draft\", \"summarize\")        # arrow: draft -> summarize\n",
        "g.add_edge(\"summarize\", END)            # arrow: summarize -> END (stop)\n",
        "\n",
        "# 5) Compile the graph into an \"app\" you can run\n",
        "app = g.compile()\n",
        "\n",
        "# 6) Run it once\n",
        "final_state = app.invoke({\"topic\": \"LangGraph basics\"})\n",
        "print(\"=== FINAL STATE ===\")\n",
        "for k, v in final_state.items():\n",
        "    print(f\"{k.upper()}:\\n{v}\\n\")"
      ],
      "metadata": {
        "id": "sdFioktr-eR3"
      },
      "id": "sdFioktr-eR3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langgraph components used\n",
        "\n",
        "`StateGraph(dict)`: the graph builder. Here, it defines a workflow where state is always a Python dict.\n",
        "\n",
        "`add_node(name, fn)`: registers a node function.\n",
        "\n",
        "`set_entry_point(name)`: sets the start node.\n",
        "\n",
        "`add_edge(from, to)`: connects two nodes (arrows).\n",
        "\n",
        "`END`: a special label telling the graph to stop.\n",
        "\n",
        "`compile()`: freezes the builder into an executable app. Produces an app that uses the Runnable interface."
      ],
      "metadata": {
        "id": "Z3darehO_IF2"
      },
      "id": "Z3darehO_IF2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversational Graph with Tools"
      ],
      "metadata": {
        "id": "SpHrQXFv_iGO"
      },
      "id": "SpHrQXFv_iGO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a Conversational Chatbot with Messages in the state and have the Agents use tools."
      ],
      "metadata": {
        "id": "6mOn__0hAel-"
      },
      "id": "6mOn__0hAel-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Define the tools"
      ],
      "metadata": {
        "id": "z5SOFzud5KnY"
      },
      "id": "z5SOFzud5KnY"
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Evaluate basic math expressions like '2 + 2 * 5'.\"\"\"\n",
        "    try:\n",
        "        return str(eval(expression))\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "@tool\n",
        "def search_web(query: str) -> str:\n",
        "    \"\"\"Fake web search that returns tiny helpful snippets.\"\"\"\n",
        "    data = {\n",
        "        \"LangGraph\": \"LangGraph builds stateful LLM workflows as graphs.\",\n",
        "        \"MMR\": \"Maximal Marginal Relevance balances relevance & diversity.\"\n",
        "    }\n",
        "    return data.get(query, f\"No results for '{query}'\")\n",
        "\n",
        "tools = [calculator, search_web]"
      ],
      "metadata": {
        "id": "Z7AsWEXv5NNg"
      },
      "id": "Z7AsWEXv5NNg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the agent graph"
      ],
      "metadata": {
        "id": "cq77lN875a-P"
      },
      "id": "cq77lN875a-P"
    },
    {
      "cell_type": "code",
      "source": [
        "llm_tools = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, api_key=key).bind_tools(tools)\n",
        "\n",
        "def call_model(state: MessagesState) -> dict:\n",
        "    # state[\"messages\"] is a Python list of messages (HumanMessage/AIMessage/ToolMessage)\n",
        "    ai_reply = llm_tools.invoke(state[\"messages\"])\n",
        "    # Return a dict with \"messages\": [<new message>] so LangGraph appends it\n",
        "    return {\"messages\": [ai_reply]}\n",
        "\n",
        "# Node: \"tools\" — executes the requested tool(s) and returns ToolMessage(s)\n",
        "tools_node = ToolNode(tools)\n",
        "\n",
        "# Router: look at the last AIMessage — did it ask for tools?\n",
        "def needs_tools_router(state: MessagesState) -> str:\n",
        "    msgs = state.get(\"messages\", [])\n",
        "    if not msgs:\n",
        "        return END\n",
        "    last = msgs[-1]\n",
        "    # Some providers put tool calls on .tool_calls, others in .additional_kwargs[\"tool_calls\"]\n",
        "    if getattr(last, \"tool_calls\", None):\n",
        "        return \"tools\"\n",
        "    if getattr(last, \"additional_kwargs\", {}).get(\"tool_calls\"):\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "# Build the graph over MessagesState\n",
        "ag = StateGraph(MessagesState)\n",
        "ag.add_node(\"call_model\", call_model)\n",
        "ag.add_node(\"tools\", tools_node)\n",
        "\n",
        "ag.set_entry_point(\"call_model\")\n",
        "# If \"call_model\" says tools are needed, go to \"tools\"; otherwise, END\n",
        "ag.add_conditional_edges(\"call_model\", needs_tools_router, {\"tools\": \"tools\", END: END})\n",
        "# After \"tools\" run, go back to \"call_model\" to continue the dialogue\n",
        "ag.add_edge(\"tools\", \"call_model\")\n",
        "\n",
        "# Optional: checkpointing — store conversation per user/session\n",
        "checkpointer = MemorySaver()\n",
        "agent_app = ag.compile(checkpointer=checkpointer)\n",
        "\n",
        "print(\"✅ Agent graph ready\")"
      ],
      "metadata": {
        "id": "s6ZL5SJ45dZ2"
      },
      "id": "s6ZL5SJ45dZ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now let's test it out\n",
        "\n",
        "by running a multi-turn session (with streaming)"
      ],
      "metadata": {
        "id": "J7W_k8uy5wt1"
      },
      "id": "J7W_k8uy5wt1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Each user/session gets a thread_id for checkpointing\n",
        "cfg = {\"configurable\": {\"thread_id\": \"user-001\"}}\n",
        "\n",
        "# Turn 1: user asks something that needs calculator and search\n",
        "print(\"=== STREAM (Turn 1) ===\")\n",
        "steps = agent_app.stream({\"messages\": [HumanMessage(content=\"Compute 2+2*5, then search LangGraph.\")]}, config=cfg)\n",
        "for step in steps:\n",
        "    # step is a dict with one key (the node that just ran), e.g. {\"call_model\": {...}}, {\"tools\": {...}}\n",
        "    print(list(step.keys()))\n",
        "\n",
        "# Turn 2: follow-up — the graph continues from saved state (checkpoint)\n",
        "print(\"\\n=== STREAM (Turn 2) ===\")\n",
        "steps = agent_app.stream({\"messages\": [HumanMessage(content=\"Summarize what you found in 2 bullets.\")]}, config=cfg)\n",
        "for step in steps:\n",
        "    print(list(step.keys()))\n",
        "\n",
        "# Inspect the latest messages\n",
        "curr = agent_app.get_state(cfg)\n",
        "print(\"\\n=== LAST 6 MESSAGES ===\")\n",
        "for m in curr.values[\"messages\"][-6:]:\n",
        "    print(m.type.upper()+\":\", getattr(m, \"content\", \"\"))"
      ],
      "metadata": {
        "id": "9hYArIGI52T3"
      },
      "id": "9hYArIGI52T3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How it works?\n",
        "\n",
        "A lot is happening in this Conversational Graph with tools. It can be a bit overwhelming. Let's go through this one by one."
      ],
      "metadata": {
        "id": "tSjbTALtEcWH"
      },
      "id": "tSjbTALtEcWH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're building a Conversational Graph with tools. The state will be `MessagesState` which will contain a list of mentions. This is an inbuilt state by Langgraph to allow for the ease of storing messages (HumanMessage, AIMessage, SystemMessage) as the state.\n",
        "\n",
        "```python\n",
        "ag = StateGraph(MessagesState)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "QYQRYDyyEtpD"
      },
      "id": "QYQRYDyyEtpD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `call_model`: runs the LLM against the conversation `(state[\"messages\"])`, and returns the new `AIMessage`.\n",
        "\n",
        "> You'd be right to think if the Node is returning the state with the same key but with a different value (here, `AIMessage`) the previous value(s) is overwritten, right? That's true but `MessagesState` has a special reducer on the `messages` key called `add_messages`. So, when a node returns `{\"messages\": [ai_reply]}`, LangGraph merges that into the existing state by extending the list (not replacing it).\n",
        "\n",
        "2. If the llm thinks we need tools for the user query a `tool_calls` attribute is added to the `AIMessage`.\n",
        "\n",
        "3. `add_conditional_edges`: In this part, once out of the `call_model`, we are passing the state to the `needs_tools_router`, if the node returns \"tools\", we are adding an edge to the `tools_node`, if not we're ending the graph there using `END`. This is declared once at the build time.\n",
        "\n",
        "4. `needs_tools_router`: This checks the last message in the state, `AIMessage` (result of the node `call_model`) if it contains an attribute called `tool_calls`, if so then we return \"tools\".\n",
        "\n",
        "5. `tools_node`: Here we use `ToolNode(tools)` a ready-made node that inspects the last `AIMessage` for tool calls, executes them, and produces `ToolMessage`(s). This is appended to the state (messages).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gYbUFMPtEmxU"
      },
      "id": "gYbUFMPtEmxU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, `call_model` appends a new `AIMessage` to `MessagesState.messages` via the state's inbuilt `add_messages` reducer; if that `AIMessage` includes `tool_calls`, the build-time `add_conditional_edges` sends control (via `needs_tools_router`) to `ToolNode`, which executes the requested tools and appends corresponding `ToolMessages`, then loops back to call_model—and when no `tool_calls` are present, the router directs the graph to `END`."
      ],
      "metadata": {
        "id": "CR43jkSNHPrv"
      },
      "id": "CR43jkSNHPrv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checkpointer and MemorySaver?"
      ],
      "metadata": {
        "id": "P07Dg8HhHpE9"
      },
      "id": "P07Dg8HhHpE9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpointer:\n",
        "- A persistence layer that saves and restores the graph state (e.g., MessagesState) between runs/turns so you can resume exactly where you left off (multi-turn, interruptions, retries).\n",
        "- Enables long-lived agents: In our case Turn 2 doesn’t need the full history; it’s loaded from storage. Instead of storing the state in any backend and retrieving it for subsequent turns and appending it to each turn, this allows to persist the state in-memory and automatically loads the history from the storage.\n",
        "\n",
        "MemorySaver:\n",
        "- An in-memory checkpointer—fast and simple for local/dev. For production use Redis using something like langgraph-checkpoint-redis.\n",
        "\n",
        "\n",
        "**How do you use it?**\n",
        "\n",
        "Provide a checkpointer at compile time and a `thread_id` at run time:\n",
        "\n",
        "```python\n",
        "checkpointer = MemorySaver()\n",
        "agent_app = ag.compile(checkpointer=checkpointer)\n",
        "cfg = {\"configurable\": {\"thread_id\": \"user-001\"}}\n",
        "agent_app.stream({\"messages\": [HumanMessage(content=\"...\")]}, config=cfg)\n",
        "```\n",
        "\n",
        "- `get_state(cfg)` inspects the latest persisted state for the given configuration.\n",
        "\n",
        "- `thread_id` defines the session identity. If a different value is given, a new conversation with clean state happens, if an existing value is given, state is persisted (In our case, the state is messages, so the thread_id becomes user session)."
      ],
      "metadata": {
        "id": "t4A-5oChIcAD"
      },
      "id": "t4A-5oChIcAD"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}