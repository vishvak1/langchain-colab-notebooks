{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ef76c6-59c1-402f-8627-c38576abb116",
   "metadata": {},
   "source": [
    "# Vector Stores & Retrievers (Embeddings + Retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad14befa-74c1-40b1-b649-5a8b50feab41",
   "metadata": {},
   "source": [
    "What are Vector Stores and Retrievers?\n",
    "\n",
    "- Embeddings turn text into numeric vectors.\n",
    "- A Vector Store indexes those vectors for fast similarity search.\n",
    "- A Retriever is a clean interface that, given a user query, returns the most relevant Document chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e3710-c510-46ba-9e8d-8b5939cef12c",
   "metadata": {},
   "source": [
    "What we'll cover:\n",
    "\n",
    "- Building embeddings and a FAISS vector store\n",
    "- Basic similarity search & the Retriever interface\n",
    "- Score thresholds, Maximal Marginal Relevance (MMR)\n",
    "- Metadata filtering\n",
    "- Lightweight retrieval compression\n",
    "- A tiny end-to-end RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3656e-77f0-48ea-8b33-9b390de82e24",
   "metadata": {},
   "source": [
    "Run this bootstrap cell before running any subsequent cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aaf190a-36d3-48e4-8dc6-cdefe7fcb06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ready: Embeddings + FAISS + Chat model\n"
     ]
    }
   ],
   "source": [
    "# Environment & imports\n",
    "import os, json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise RuntimeError(\"Missing OPENAI_API_KEY in .env\")\n",
    "\n",
    "# Embeddings + vector stores\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Prompts & model for RAG demo\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "\n",
    "# Documents & splitting (we'll create toy docs below)\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"✅ Ready: Embeddings + FAISS + Chat model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145586dd-629e-461f-95fc-4edf0f2098f0",
   "metadata": {},
   "source": [
    "Let's create moderately sized chunks using `RecursiveTextSplitter` we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa425281-f65c-443a-a402-041cac70f6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912,\n",
       " [Document(metadata={'producer': 'Adobe PDF Library 24.5.96', 'creator': 'Acrobat PDFMaker 24 for Word', 'creationdate': '2025-01-27T08:44:50-06:00', 'author': 'Joel Youvan', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2025-01-27T08:44:55-06:00', 'sourcemodified': 'D:20250127144434', 'subject': '', 'title': '', 'rgid': 'PB:388414789_AS:11431281305729682@1737989125997', 'source': 'rag_info.pdf', 'total_pages': 58, 'page': 0, 'page_label': '1'}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/388414789'),\n",
       "  Document(metadata={'producer': 'Adobe PDF Library 24.5.96', 'creator': 'Acrobat PDFMaker 24 for Word', 'creationdate': '2025-01-27T08:44:50-06:00', 'author': 'Joel Youvan', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2025-01-27T08:44:55-06:00', 'sourcemodified': 'D:20250127144434', 'subject': '', 'title': '', 'rgid': 'PB:388414789_AS:11431281305729682@1737989125997', 'source': 'rag_info.pdf', 'total_pages': 58, 'page': 0, 'page_label': '1'}, page_content='Retrieval-Augmented Generation (RAG): Advancing AI with Dynamic Knowledge\\nIntegration\\nPreprint · January 2025')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader(\"rag_info.pdf\")\n",
    "pdf_docs = pdf_loader.load() \n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=120, chunk_overlap=20)\n",
    "docs = splitter.split_documents(pdf_docs)\n",
    "len(docs), docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc700092-9a69-4126-9602-d5eca5efa0c8",
   "metadata": {},
   "source": [
    "## Build embeddings + FAISS vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6466014-a41e-4b53-82ab-2d8b26ac6d69",
   "metadata": {},
   "source": [
    "Embedding is converting chunks into a vector that holds the semantic context of the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01802e59-50d9-4336-a9a0-635b3f861b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed vectors: 912\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # fast & inexpensive for learning\n",
    "vs = FAISS.from_documents(docs, embedding=embeddings)\n",
    "\n",
    "print(\"Indexed vectors:\", vs.index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e09eb-16af-4b71-a1f0-0e9248c61865",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a17c8-e3a2-4527-bad6-4d1d8860505d",
   "metadata": {},
   "source": [
    "1. Each chunk is embedded into a vector.\n",
    "2. FAISS uses cosine-similarity to store the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cdbf97-2012-4483-8cef-814b5abc2bbd",
   "metadata": {},
   "source": [
    "## Basic Similarity Search and k-size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e25d1-12fe-40ae-816c-df37f89968cd",
   "metadata": {},
   "source": [
    "Top-k or k-size defines the number of chunks being returned. If k-size is 3, top 3 results based on the user query will be yielded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f7abe-4dda-4941-8255-e48fe9c96136",
   "metadata": {},
   "source": [
    "A basic similarity search finds the most conceptually similar items from a collection by comparing their numerical representations (vector embeddings). For this purpose cosine similarity is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d2a374a-bf08-42c9-8e5f-bcb94cf3121e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- HIT 1 ---\n",
      "searching and retrieving the most relevant information grows,\n",
      "metadata: {'producer': 'Adobe PDF Library 24.5.96', 'creator': 'Acrobat PDFMaker 24 for Word', 'creationdate': '2025-01-27T08:44:50-06:00', 'author': 'Joel Youvan', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2025-01-27T08:44:55-06:00', 'sourcemodified': 'D:20250127144434', 'subject': '', 'title': '', 'rgid': 'PB:388414789_AS:11431281305729682@1737989125997', 'source': 'rag_info.pdf', 'total_pages': 58, 'page': 24, 'page_label': '25'}\n",
      "\n",
      "--- HIT 2 ---\n",
      "queries. \n",
      "• Optimizing indexing techniques (e.g., approximate nearest neighbor search) \n",
      "for faster lookups.\n",
      "metadata: {'producer': 'Adobe PDF Library 24.5.96', 'creator': 'Acrobat PDFMaker 24 for Word', 'creationdate': '2025-01-27T08:44:50-06:00', 'author': 'Joel Youvan', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2025-01-27T08:44:55-06:00', 'sourcemodified': 'D:20250127144434', 'subject': '', 'title': '', 'rgid': 'PB:388414789_AS:11431281305729682@1737989125997', 'source': 'rag_info.pdf', 'total_pages': 58, 'page': 25, 'page_label': '26'}\n",
      "\n",
      "--- HIT 3 ---\n",
      "o Dividing large datasets across multiple nodes for distributed search \n",
      "performance.\n",
      "metadata: {'producer': 'Adobe PDF Library 24.5.96', 'creator': 'Acrobat PDFMaker 24 for Word', 'creationdate': '2025-01-27T08:44:50-06:00', 'author': 'Joel Youvan', 'comments': '', 'company': '', 'keywords': '', 'moddate': '2025-01-27T08:44:55-06:00', 'sourcemodified': 'D:20250127144434', 'subject': '', 'title': '', 'rgid': 'PB:388414789_AS:11431281305729682@1737989125997', 'source': 'rag_info.pdf', 'total_pages': 58, 'page': 16, 'page_label': '17'}\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I search relevant chunks efficiently?\"\n",
    "hits = vs.similarity_search(query, k=3)  # returns Documents\n",
    "for i, d in enumerate(hits, 1):\n",
    "    print(f\"\\n--- HIT {i} ---\")\n",
    "    print(d.page_content)\n",
    "    print(\"metadata:\", d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86794554-94dc-446e-954e-6732a7c5a240",
   "metadata": {},
   "source": [
    "## Similarity Search with scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fc3f9c-6785-49b6-97f8-00a7c6db0a1e",
   "metadata": {},
   "source": [
    "Sometimes you want both the documents and their similarity scores, and to filter weak matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08ddd0a1-cde1-44b5-84cd-800ef1fb7a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned 5; kept 5 under threshold 0.6\n",
      "\n",
      "<class 'tuple'>\n",
      "score=0.461 :: Key Advantages of RAG Over Traditional Models:...\n",
      "score=0.554 :: paper explores the fundamentals of RAG, its technical implementation, key...\n",
      "score=0.565 :: 6 \n",
      " \n",
      "Core Components of RAG \n",
      "At a high level, RAG consists of two primary components that work in ta...\n",
      "score=0.578 :: reliable and domain-specific insights. These advantages make RAG particularly...\n",
      "score=0.590 :: Challenges of RAG Compared to Traditional Models:...\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the advantages of RAG?\"\n",
    "scored = vs.similarity_search_with_score(query, k=5)  # list of (Document, score)\n",
    "# Lower score means closer (FAISS returns distances; wrapper converts appropriately)\n",
    "threshold = 0.6  # tune this per model/index; smaller is stricter if using L2 distance\n",
    "kept = [(d, s) for d, s in scored if s <= threshold]\n",
    "\n",
    "print(f\"Returned {len(scored)}; kept {len(kept)} under threshold {threshold}\\n\")\n",
    "\n",
    "print(type(scored[0]))\n",
    "\n",
    "for d, s in kept:\n",
    "    print(f\"score={s:.3f} :: {d.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb208340-7c1b-49f9-b866-1c00104d5d6d",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f60c9-891e-4dad-b12e-431a5a090771",
   "metadata": {},
   "source": [
    "The `.similarity_search_with_score` returns a list of tuples, for each tuple the first value is the Document object and the second is the score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c761d255-cbb1-41cb-9901-0237db2833a8",
   "metadata": {},
   "source": [
    "## Maximal Marginal Relevance (MMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e36b3-6135-40df-9c34-533863937f41",
   "metadata": {},
   "source": [
    "Maximal Marginal Relevance (MMR) is a method used to retrieve a list of items that are both relevant to a query and diverse from one another.\n",
    "\n",
    "In a similarity search the top retrieved documents are similar to one another. If your document contains similar verbose about a topic chances are only those are returned in a similarity search. This makes the retrived chunks too similar in meaning.\n",
    "\n",
    "MMR focuses diversifying these results.\n",
    "\n",
    "1. First, it selects the single most relevant item for your query.\n",
    "2. Then, for every subsequent selection, it picks the item that offers the best trade-off between being relevant to the query and being different from the items already selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce6bc477-af35-4993-b386-aec8aab5fa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MMR 1 ---\n",
      "o Example: Elasticsearch, Apache Solr \n",
      "• Vector Indexing (for dense retrieval):\n",
      "\n",
      "--- MMR 2 ---\n",
      "2. Fundamentals of Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "--- MMR 3 ---\n",
      "o Stores document embeddings in a high-dimensional space. \n",
      "o Example: FAISS, Annoy \n",
      "• Sharding and Partitioning:\n",
      "\n",
      "--- MMR 4 ---\n",
      "Retrieval (DPR) encode text into embeddings that can capture \n",
      "contextual meaning. \n",
      "o Advantages:\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain retrieval and vector stores.\"\n",
    "mmr_hits = vs.max_marginal_relevance_search(query, k=4, fetch_k=10, lambda_mult=0.5)\n",
    "for i, d in enumerate(mmr_hits, 1):\n",
    "    print(f\"\\n--- MMR {i} ---\")\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ce90c-c5c4-41ae-a3ca-622261acd670",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92b7a5-76eb-44b5-b710-83f9aec2e40d",
   "metadata": {},
   "source": [
    "- `fetch_k` is the candidate pool. This determines how many documents will retrieved using only a similarity search on which the MMR's diversification logic is applied.\n",
    "    - If your fetch_k is too small then there will be less diversity between the results.\n",
    "- `lambda_mult` controls the trade-off between relevance (similarity to the query) and diversity (dissimilarity from already selected documents).\n",
    "    - For a value $1.0$, there is no diversification of results and the similarity search results are returned without any additional MMR logic.\n",
    "    - For a value $0.0$, This would maximize diversity, potentially returning results that are very different from each other but not very relevant to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cdd8ee-cf9d-49b1-8e9b-3f143363267d",
   "metadata": {},
   "source": [
    "## Vector store as Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d60f8-9d44-405d-9a90-57ea7fcb74d3",
   "metadata": {},
   "source": [
    "This converts the Vector store into a retriever. It uses similarity search to find documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ff752c-2e1f-4ee1-a449-7b2ec3400876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/sl1lxn9912z0v92cfhwp2h600000gn/T/ipykernel_53589/83117429.py:5: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  for d in retriever.get_relevant_documents(\"What interface returns relevant docs for a query?\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The retrieval system relies on various methods to identify the most relevant data \n",
      "based on the input query:\n",
      "- queries to ensure more relevant and precise results.\n",
      "- Key Advancements: \n",
      "1. Interactive Query Refinement:\n"
     ]
    }
   ],
   "source": [
    "# Basic retriever\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Try it\n",
    "for d in retriever.get_relevant_documents(\"What interface returns relevant docs for a query?\"):\n",
    "    print(\"-\", d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224acba-6e05-4080-a07d-baf0f5fbac03",
   "metadata": {},
   "source": [
    "## ContextualCompressionRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28722bd4-c8cf-46ff-85ee-5e538df1b341",
   "metadata": {},
   "source": [
    "ContextualCompressionRetriever is a LangChain retriever that \"compresses\" retrieved documents to improve the quality of a Large Language Model's (LLM) response in a RAG (Retrieval Augmented Generation) system.\n",
    "\n",
    "It also removes irrelevant context from the retrieved documents to the user's query. Hence optimising the number of tokens processed for a given user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "295820dc-29bd-4219-8a0f-09bda4c0d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "# Simple embeddings-based sentence filter\n",
    "compressor = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.6)\n",
    "compressed_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=retriever,\n",
    "    base_compressor=compressor\n",
    ")\n",
    "\n",
    "docs_compressed = compressed_retriever.get_relevant_documents(\"What is a Vector embedding and why is it needed?\")\n",
    "for i, d in enumerate(docs_compressed, 1):\n",
    "    print(f\"\\n--- COMPRESSED {i} ---\")\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe12bc3-914a-4fea-bb2a-8989c1bba689",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68038c48-6ddb-4e60-926f-b8ba6d466c8c",
   "metadata": {},
   "source": [
    "1. Initial Retrieval: The `ContextualCompressionRetriever` first uses its `base_retriever` to fetch a set of documents based on the user's query. This step typically focuses on recall (getting a broad set of potentially relevant documents), which can often include a lot of irrelevant \"fluff.\"\n",
    "2. Compression: The initially retrieved documents are then passed to the `document_compressor`. This component processes the documents, filtering out irrelevant content and keeping only the information most pertinent to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b033a747-98f3-4961-9e44-25bed4d4021b",
   "metadata": {},
   "source": [
    "The compression can be done in a few ways:\n",
    "\n",
    "- **LLM-based compression:** A document compressor like `LLMChainExtractor` uses an LLM to read through each document and extract only the sentences or passages that are highly relevant to the query.\n",
    "\n",
    "- **Embedding-based filtering:** A compressor like `EmbeddingsFilter` uses embeddings to filter out entire documents if their similarity to the query falls below a certain threshold. _(This is the one we use in the above example)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d253c71-6b51-4378-83a5-1d20f6179f8f",
   "metadata": {},
   "source": [
    "## A tiny RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6c8d1-29d4-456b-b924-8df6d0630256",
   "metadata": {},
   "source": [
    "Let's implement a tiny RAG to get to know more about Embeddings and Retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97e5a10c-84a1-45c2-97ad-16f1f070e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RAG ANSWER ---\n",
      "\n",
      "RAG reduces hallucinations by retrieving data to check outputs, which reduces the likelihood of hallucinations and enhances factual correctness.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer using ONLY the provided context. If unsure, say you don't know.\\n\\nContext:\\n{context}\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "def stuff_context(question: str, retriever):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "    return {\"context\": context, \"question\": question}\n",
    "\n",
    "rag_chain = (\n",
    "    # adapter that builds the input dict expected by the prompt\n",
    "    (lambda x: stuff_context(x[\"question\"], retriever))\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"\\n--- RAG ANSWER ---\\n\")\n",
    "print(rag_chain.invoke({\"question\": \"How RAG reduces hallucinations?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e7fc4-e21f-40de-9bf9-07376c531a3f",
   "metadata": {},
   "source": [
    "## Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196a6943-10ae-4bfa-a619-7eb1e8bbed0c",
   "metadata": {},
   "source": [
    "Try to implment MMR in the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e6ec5-24c2-4ce7-8c1e-921b352aa5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
