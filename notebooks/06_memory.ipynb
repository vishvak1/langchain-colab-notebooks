{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1ff9e678-7da5-4877-b675-c563c900d724",
      "metadata": {
        "id": "1ff9e678-7da5-4877-b675-c563c900d724"
      },
      "source": [
        "# Memory (Conversational / State Memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31291e14-6051-4788-9b37-d8dcab57b1f0",
      "metadata": {
        "id": "31291e14-6051-4788-9b37-d8dcab57b1f0"
      },
      "source": [
        "Memory helps your app carry state across turns (conversation history, summaries, slots), so the model can stay on topic, avoid repetition, and reference prior facts.\n",
        "\n",
        "In LangChain v0.3 you can do memory in a few straightforward ways:\n",
        "- Manual history (a list of messages) with MessagesPlaceholder\n",
        "- `RunnableWithMessageHistory` to auto-log and replay history per session\n",
        "- Windowed memory (only keep the last N turns)\n",
        "- Summary memory (keep a running summary + recent messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bootstrap"
      ],
      "metadata": {
        "id": "-r-3rhCKPqf8"
      },
      "id": "-r-3rhCKPqf8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚓--- Before proceeding futher it is very important you do the following: --- 👾\n",
        "\n",
        "Select the 🗝 (key) icon in the left pane and include your OpenAI Api key with Name as \"OPENAPI_KEY\" and value as the key, and grant it notebook access in order to be able to run this notebook."
      ],
      "metadata": {
        "id": "YP3P3V-9L9F8"
      },
      "id": "YP3P3V-9L9F8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeddf0b2-fc6a-4479-8d39-6dfaa715bef9"
      },
      "source": [
        "Run the below two cells in the order they are in, before running further cells. Wait till a number appears in place of '*' or '[ ]'. Below the cell you should see \"✅ Ready: Chat model initialized\""
      ],
      "id": "aeddf0b2-fc6a-4479-8d39-6dfaa715bef9"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-openai langchain-community"
      ],
      "metadata": {
        "id": "V37607__LK8B"
      },
      "execution_count": null,
      "outputs": [],
      "id": "V37607__LK8B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c34ec7a-35c8-48a2-9eea-fd5e8c164fe2",
      "metadata": {
        "id": "9c34ec7a-35c8-48a2-9eea-fd5e8c164fe2"
      },
      "outputs": [],
      "source": [
        "# Environment & imports\n",
        "from google.colab import userdata\n",
        "\n",
        "key = userdata.get('OPENAI_API_KEY')  # returns None if not granted\n",
        "if not key:\n",
        "    raise RuntimeError(\"Set OPENAI_API_KEY in a .env file next to this notebook.\")\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# For automatic chat histories with LCEL\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, api_key=key)\n",
        "print(\"✅ Ready: Chat model initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c72d5f76-2fb9-4aa0-a7ac-9eb870c177a1",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "c72d5f76-2fb9-4aa0-a7ac-9eb870c177a1"
      },
      "source": [
        "## Manual Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec242aa2-5cc5-4aa7-a86f-2643b6d16412",
      "metadata": {
        "id": "ec242aa2-5cc5-4aa7-a86f-2643b6d16412"
      },
      "source": [
        "LLMs don't retain memory in the same way humans retain previous conversations. LLMs are sophisticated mathematical functions that figure out what word comes next for a given sequence of words.\n",
        "\n",
        "In order to make LLMs retain history of previous message to function as a conversational chatbot, there are a number of ways to include the previous messages.\n",
        "\n",
        "The most simplest of them is to keep a list of history and pass it every turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c298cea6-4bce-4480-a8b7-9cb9e6913709",
      "metadata": {
        "id": "c298cea6-4bce-4480-a8b7-9cb9e6913709"
      },
      "outputs": [],
      "source": [
        "# We’ll define a prompt that accepts a history placeholder + new user input\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a concise assistant. If context is insufficient, ask a clarifying question.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"user\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Start empty conversation history\n",
        "history: list[BaseMessage] = []\n",
        "\n",
        "# Turn 1\n",
        "user_input_1 = \"I'm planning a weekend trip. Suggest 3 destinations near Bangalore.\"\n",
        "answer1 = chain.invoke({\"history\": history, \"user_input\": user_input_1})\n",
        "history.append(HumanMessage(content=user_input_1))\n",
        "history.append(AIMessage(content=answer1))\n",
        "\n",
        "# Turn 2\n",
        "user_input_2 = \"Pick the best for hiking and tell me why.\"\n",
        "answer2 = chain.invoke({\"history\": history, \"user_input\": user_input_2})\n",
        "history.append(HumanMessage(content=user_input_2))\n",
        "history.append(AIMessage(content=answer2))\n",
        "\n",
        "print(\"\\n--- Turn 1 ---\\n\", answer1)\n",
        "print(\"\\n--- Turn 2 ---\\n\", answer2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad6ea68-7fd7-4171-8405-6173735c2bd3",
      "metadata": {
        "id": "9ad6ea68-7fd7-4171-8405-6173735c2bd3"
      },
      "source": [
        "Refer to `01_prompt_templates` file to understand what Messages and MessagePlaceholders are used for here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7420fd4-7426-4046-ad66-d8a519e86690",
      "metadata": {
        "id": "b7420fd4-7426-4046-ad66-d8a519e86690"
      },
      "source": [
        "### How it works"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c00aa5a-52ca-4994-a2b7-1dd54f4e7f74",
      "metadata": {
        "id": "1c00aa5a-52ca-4994-a2b7-1dd54f4e7f74"
      },
      "source": [
        "1. You keep a list of messages (history) that grows each turn.\n",
        "2. The prompt includes a `MessagesPlaceholder(\"history\")`.\n",
        "3. Each turn: append HumanMessage, call the chain, append the `AIMessage`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44860aa6-5490-42bf-8cb2-c5e31f55ace4",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "44860aa6-5490-42bf-8cb2-c5e31f55ace4"
      },
      "source": [
        "## Auto Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab2debd0-1564-4a66-9647-3d9732b8a77d",
      "metadata": {
        "id": "ab2debd0-1564-4a66-9647-3d9732b8a77d"
      },
      "source": [
        "Manually passing history is fine, but in a real app you’ll want the chain to store & fetch history automatically per user or session."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a399585c-b8d4-42ca-8e40-42545f5e18da",
      "metadata": {
        "id": "a399585c-b8d4-42ca-8e40-42545f5e18da"
      },
      "source": [
        "Here's an example of session scoped chat using an in-memory store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "654276e2-f789-43f0-8d46-5e0132bc6b24",
      "metadata": {
        "id": "654276e2-f789-43f0-8d46-5e0132bc6b24"
      },
      "outputs": [],
      "source": [
        "# A base chat prompt that expects a {question}; history will be injected automatically\n",
        "base_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Be brief and helpful.\"),\n",
        "    MessagesPlaceholder(\"history\"),\n",
        "    (\"user\", \"{question}\")\n",
        "])\n",
        "\n",
        "base_chain = base_prompt | llm | StrOutputParser()\n",
        "\n",
        "# A simple per-session store for histories\n",
        "store: dict[str, InMemoryChatMessageHistory] = {}\n",
        "\n",
        "def get_history(sess_id: str) -> BaseChatMessageHistory:\n",
        "    if sess_id not in store:\n",
        "        store[sess_id] = InMemoryChatMessageHistory()\n",
        "    return store[sess_id]\n",
        "\n",
        "# Wrap base_chain with RunnableWithMessageHistory\n",
        "chat_with_memory = RunnableWithMessageHistory(\n",
        "    base_chain,\n",
        "    get_history,\n",
        "    input_messages_key=\"question\",   # which input field is the new user message\n",
        "    history_messages_key=\"history\",  # which placeholder the chain expects\n",
        ")\n",
        "\n",
        "# Simulate two turns in the same session\n",
        "session_config = {\"configurable\": {\"session_id\": \"user-123\"}}\n",
        "print(type(session_config))\n",
        "\n",
        "resp1 = chat_with_memory.invoke({\"question\": \"Remind me what LCEL is in one sentence.\"}, config=session_config)\n",
        "resp2 = chat_with_memory.invoke({\"question\": \"Great. Now give me two practical uses.\"},  config=session_config)\n",
        "\n",
        "print(\"\\n--- Session user-123 ---\")\n",
        "print(resp1)\n",
        "print(resp2)\n",
        "\n",
        "# A different session starts with a clean slate\n",
        "resp_other = chat_with_memory.invoke({\"question\": \"What is LCEL?\"}, config={\"configurable\": {\"session_id\": \"guest\"}})\n",
        "print(\"\\n--- Session guest ---\")\n",
        "print(resp_other)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54b633f8-b8b5-4d05-aabc-ce9ba68131c9",
      "metadata": {
        "id": "54b633f8-b8b5-4d05-aabc-ce9ba68131c9"
      },
      "source": [
        "### How it works?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06d7693f-6931-4acf-a63e-826aa1182713",
      "metadata": {
        "id": "06d7693f-6931-4acf-a63e-826aa1182713"
      },
      "source": [
        "`RunnableWithMessageHistory` requires the following inputs:\n",
        "1. Any LCEL runnable that uses the prompt template with `MessagesPlaceholder`.\n",
        "2. A callable that accepts a string (session id) as input and returns a `BaseChatMessageHistory` implementation.\n",
        "3. `input_messages_key` which key is used in the invoke for the user query.\n",
        "4. `history_messages_key` which key is used in the `MessagesPlaceholder`.\n",
        "5. A config value in the `.invoke`. The config must be passed as `{\"configurable\": {\"session_id\": \"<id>\"}}` those keys are fixed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45139714-cdf3-4e33-9e1c-a9b232bc9a93",
      "metadata": {
        "id": "45139714-cdf3-4e33-9e1c-a9b232bc9a93"
      },
      "source": [
        "On each call, the `RunnableWithMessageHistory`:\n",
        "- Derives a session identifier from the config dictionary.\n",
        "- Retrieves a `BaseChatMessageHistory` (here, `InMemoryChatMessageHistory`) which exposes `messages`, `add_user_message`, `add_ai_message`.\n",
        "- Creates inputs by merging the caller’s inputs and history_messages_key: history.messages.\n",
        "- Delegates to `runnable.invoke(...`.\n",
        "- Persists the turn by writing user/AI messages back to the history."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7eb1ea9-5067-4ded-99f2-53d14b315ea6",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "b7eb1ea9-5067-4ded-99f2-53d14b315ea6"
      },
      "source": [
        "## Windowed Memory (keep only the last K messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cb63e34-7ebf-485f-9728-34b806d5fc9b",
      "metadata": {
        "id": "7cb63e34-7ebf-485f-9728-34b806d5fc9b"
      },
      "source": [
        "Long histories get expensive. A common trick is to window: keep only the last K messages (or last K turns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f2ce0c4-363f-465a-b9ee-adb715a280a9",
      "metadata": {
        "id": "6f2ce0c4-363f-465a-b9ee-adb715a280a9"
      },
      "outputs": [],
      "source": [
        "def last_n_messages(history: list[BaseMessage], n: int = 6) -> list[BaseMessage]:\n",
        "    # keep only the tail of the conversation\n",
        "    return history[-n:] if len(history) > n else history\n",
        "\n",
        "# Build a windowed chain: compute a trimmed history before calling the model\n",
        "windowed_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are helpful and concise.\"),\n",
        "    MessagesPlaceholder(\"history\"),\n",
        "    (\"user\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "def with_window(inputs):\n",
        "    full_history = inputs[\"history\"]\n",
        "    trimmed = last_n_messages(full_history, n=4)\n",
        "    return {\"history\": trimmed, \"user_input\": inputs[\"user_input\"]}\n",
        "\n",
        "windowed_chain = RunnableLambda(with_window) | windowed_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Demo\n",
        "hist = []\n",
        "hist.append(HumanMessage(content=\"Remember my favorite city is Mysuru.\")); hist.append(AIMessage(content=\"Got it!\"))\n",
        "hist.append(HumanMessage(content=\"I like hiking.\"));                     hist.append(AIMessage(content=\"Noted.\"))\n",
        "hist.append(HumanMessage(content=\"I enjoy filter coffee.\"));             hist.append(AIMessage(content=\"Nice!\"))\n",
        "\n",
        "# Now ask something: only the last 4 messages will be sent to the model\n",
        "ans = windowed_chain.invoke({\"history\": hist, \"user_input\": \"Plan a morning in my favorite city.\"})\n",
        "print(\"\\n--- Windowed Answer ---\\n\", ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7788a7df-e126-440d-9de8-8959f290645e",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "7788a7df-e126-440d-9de8-8959f290645e"
      },
      "source": [
        "## Summary Memory (rolling summary + recent messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5151e82-1254-4529-a095-d36ffe903ffe",
      "metadata": {
        "id": "e5151e82-1254-4529-a095-d36ffe903ffe"
      },
      "source": [
        "Windowed memory cannot retain the context of the previous messages, this is not useful when you're developing a chatbot."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "135ab2cf-1ae6-4a2c-bd8e-bc33056563fd",
      "metadata": {
        "id": "135ab2cf-1ae6-4a2c-bd8e-bc33056563fd"
      },
      "source": [
        "Another strategy is to keep a short summary of prior context (the “old” part), plus fresh recent messages verbatim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb94dc83-7ef0-4164-8c64-07bac6394ad1",
      "metadata": {
        "id": "fb94dc83-7ef0-4164-8c64-07bac6394ad1"
      },
      "outputs": [],
      "source": [
        "# A small summarizer chain that condenses old history + the latest turn\n",
        "summarizer_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Summarize the conversation so far in 3-4 crisp bullet points. Keep key facts.\"),\n",
        "    (\"user\", \"Existing summary:\\n{summary}\\n\\nNew messages:\\n{new_text}\\n\\nReturn only the updated summary.\")\n",
        "])\n",
        "summarizer_chain = summarizer_prompt | llm | StrOutputParser()\n",
        "\n",
        "def update_summary(summary: str, new_messages: list[BaseMessage]) -> str:\n",
        "    joined = \"\\n\".join(f\"{m.type.upper()}: {m.content}\" for m in new_messages) # This creates a string that looks like this USER: Hi\\nASSISTANT: Hello back\n",
        "    return summarizer_chain.invoke({\"summary\": summary or \"(none yet)\", \"new_text\": joined})\n",
        "\n",
        "# Conversation driver that keeps:\n",
        "#   - 'summary' for distant context\n",
        "#   - last 4 messages verbatim\n",
        "summary = \"\"\n",
        "history: list[BaseMessage] = []\n",
        "\n",
        "def ask(user_text: str):\n",
        "    global summary, history\n",
        "    history.append(HumanMessage(content=user_text))\n",
        "\n",
        "    # Build the prompt: include summary as a system hint + last K messages\n",
        "    K = 4\n",
        "    recent = history[-K:]\n",
        "\n",
        "    composed = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are helpful and concise.\"),\n",
        "        (\"system\", \"Conversation summary so far (for context): {summary}\"),\n",
        "        MessagesPlaceholder(\"recent\"),\n",
        "        (\"user\", \"{user_input}\")\n",
        "    ]) | llm | StrOutputParser()\n",
        "\n",
        "    answer = composed.invoke({\"summary\": summary, \"recent\": recent, \"user_input\": user_text})\n",
        "    history.append(AIMessage(content=answer))\n",
        "\n",
        "    # Periodically fold older messages into the summary (here: when history grows)\n",
        "    if len(history) > 8:\n",
        "        # summarize everything except the most recent K messages\n",
        "        old = history[:-K]\n",
        "        summary = update_summary(summary, old)\n",
        "        # keep only the recent window in history (we already folded the old part)\n",
        "        history[:] = history[-K:]\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Demo turns\n",
        "print(ask(\"I’m planning a 2-day trip; I love nature walks and historical places.\"))\n",
        "print(ask(\"Budget is moderate; prefer public transport.\"))\n",
        "print(ask(\"Suggest 2 itineraries near Bangalore.\"))\n",
        "print(ask(\"Pick one and list packing essentials.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b712b2ed-6983-41a7-9939-9230888442f2",
      "metadata": {
        "id": "b712b2ed-6983-41a7-9939-9230888442f2"
      },
      "source": [
        "### How it works"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7484161a-363b-4f0d-88de-34d4a12ba589",
      "metadata": {
        "id": "7484161a-363b-4f0d-88de-34d4a12ba589"
      },
      "source": [
        "- You maintain a summary string for distant context (cheap to include).\n",
        "- You keep only recent messages verbatim (last K).\n",
        "- As the conversation grows, fold older content into the summary via a small summarizer chain.\n",
        "- The main prompt includes both: summary and recent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55cb506c-b58e-42a4-8b49-cf4f9b70f683",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "55cb506c-b58e-42a4-8b49-cf4f9b70f683"
      },
      "source": [
        "## Slot/State Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08dbe9f6-5e5b-41d8-9d70-b0570ed799fb",
      "metadata": {
        "id": "08dbe9f6-5e5b-41d8-9d70-b0570ed799fb"
      },
      "source": [
        "You need to persist more information other than the chat history. You'll need the user specific information, key discoveries like user's name, email, preferences, home city. These might not be available just through summarizing conversations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bd46e71-c2ea-4d75-926e-e605144a43a8",
      "metadata": {
        "id": "3bd46e71-c2ea-4d75-926e-e605144a43a8"
      },
      "outputs": [],
      "source": [
        "# A dict acts as simple slot memory\n",
        "profile = {\"name\": \"Raghu\", \"home_city\": \"Bangalore\", \"likes\": [\"hiking\", \"filter coffee\"]}\n",
        "\n",
        "profile_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a personalized assistant.\"),\n",
        "    (\"system\", \"User profile: name={name}, home_city={home_city}, likes={likes}\"),\n",
        "    MessagesPlaceholder(\"history\"),\n",
        "    (\"user\", \"{question}\")\n",
        "])\n",
        "\n",
        "profile_chain = profile_prompt | llm | StrOutputParser()\n",
        "\n",
        "hist = []\n",
        "hist.append(HumanMessage(content=\"Remember I prefer early mornings.\")); hist.append(AIMessage(content=\"Noted.\"))\n",
        "\n",
        "print(profile_chain.invoke({\n",
        "    \"name\": profile[\"name\"],\n",
        "    \"home_city\": profile[\"home_city\"],\n",
        "    \"likes\": \", \".join(profile[\"likes\"]),\n",
        "    \"history\": hist,\n",
        "    \"question\": \"Plan a Saturday morning activity I’d enjoy.\"\n",
        "}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eaa55bd-c18a-4b89-9fe6-7dd12468f2fa",
      "metadata": {
        "id": "1eaa55bd-c18a-4b89-9fe6-7dd12468f2fa"
      },
      "source": [
        "### How it works"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e1bbdea-0367-4283-b9ff-7969b00d68e5",
      "metadata": {
        "id": "7e1bbdea-0367-4283-b9ff-7969b00d68e5"
      },
      "source": [
        "- Store stable facts outside chat as a dict/DB.\n",
        "- Inject them into the prompt as system context.\n",
        "- Use history for ephemeral, conversational detail."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}